{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josephthomaa/ML-Notebooks/blob/main/Llama_finetuned_quantized_JT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "git clone https://github.com/PromtEngineer/localGPT.git\n",
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZM8cX3NK9IS",
        "outputId": "66482eee-7ccc-4e29-8ad3-24b6bd55c704"
      },
      "id": "5ZM8cX3NK9IS",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "localGPT\n",
            "sample_data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'localGPT'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "pip install -r localGPT/requirements.txt\n"
      ],
      "metadata": {
        "id": "WSxsJ3-rLK8G"
      },
      "id": "WSxsJ3-rLK8G",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "\n",
        "\n",
        "# rm -r localGPT/DB/\n",
        "python localGPT/ingest.py"
      ],
      "metadata": {
        "id": "QebiFrSjLTGx"
      },
      "id": "QebiFrSjLTGx",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e4c5aae2-41bb-4089-b4af-d2feb3bcb48a",
      "metadata": {
        "id": "e4c5aae2-41bb-4089-b4af-d2feb3bcb48a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import click\n",
        "import torch\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  # for streaming response\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "from localGPT.prompt_template_utils import get_prompt_template\n",
        "\n",
        "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.vectorstores import Chroma\n",
        "from transformers import (\n",
        "    GenerationConfig,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "# from localGPT.load_models import (\n",
        "#     load_quantized_model_gguf_ggml,\n",
        "#     load_quantized_model_qptq,\n",
        "#     load_full_model,\n",
        "# )\n",
        "\n",
        "# from localGPT.constants import (\n",
        "#     EMBEDDING_MODEL_NAME,\n",
        "#     PERSIST_DIRECTORY,\n",
        "#     MODEL_ID,\n",
        "#     MODEL_BASENAME,\n",
        "#     MAX_NEW_TOKENS,\n",
        "#     MODELS_PATH,\n",
        "# )\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0080ef43-f42c-4f8a-8582-0d5c11c8b5a2",
      "metadata": {
        "id": "0080ef43-f42c-4f8a-8582-0d5c11c8b5a2"
      },
      "outputs": [],
      "source": [
        "# Finetuned quantized model\n",
        "\n",
        "# MODEL_ID = \"ggml-model-q4_k_m_new.gguf\"\n",
        "# MODEL_BASENAME = \"ggml-model-q4_k_m_new.gguf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "05f343a3-1c01-4362-a1c0-5bf93cf32735",
      "metadata": {
        "id": "05f343a3-1c01-4362-a1c0-5bf93cf32735"
      },
      "outputs": [],
      "source": [
        "# Pretrained quantized model\n",
        "\n",
        "MODEL_ID = \"TheBloke/Llama-2-7b-Chat-GGUF\"\n",
        "MODEL_BASENAME = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
        "EMBEDDING_MODEL_NAME = \"hkunlp/instructor-large\"\n",
        "\n",
        "import os\n",
        "\n",
        "# from dotenv import load_dotenv\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/excel.html?highlight=xlsx#microsoft-excel\n",
        "from langchain.document_loaders import CSVLoader, PDFMinerLoader, TextLoader, UnstructuredExcelLoader, Docx2txtLoader\n",
        "from langchain.document_loaders import UnstructuredFileLoader, UnstructuredMarkdownLoader\n",
        "\n",
        "\n",
        "# load_dotenv()\n",
        "ROOT_DIRECTORY = \"localGPT\"\n",
        "\n",
        "# Define the folder for storing database\n",
        "SOURCE_DIRECTORY = f\"{ROOT_DIRECTORY}/SOURCE_DOCUMENTS\"\n",
        "\n",
        "PERSIST_DIRECTORY = f\"{ROOT_DIRECTORY}/DB\"\n",
        "\n",
        "MODELS_PATH = \"./models\"\n",
        "\n",
        "# Can be changed to a specific number\n",
        "INGEST_THREADS = os.cpu_count() or 8\n",
        "\n",
        "# Define the Chroma settings\n",
        "CHROMA_SETTINGS = Settings(\n",
        "    anonymized_telemetry=False,\n",
        "    is_persistent=True,\n",
        ")\n",
        "\n",
        "# Context Window and Max New Tokens\n",
        "CONTEXT_WINDOW_SIZE = 4096\n",
        "MAX_NEW_TOKENS = CONTEXT_WINDOW_SIZE  # int(CONTEXT_WINDOW_SIZE/4)\n",
        "\n",
        "#### If you get a \"not enough space in the buffer\" error, you should reduce the values below, start with half of the original values and keep halving the value until the error stops appearing\n",
        "\n",
        "N_GPU_LAYERS = 100  # Llama-2-70B has 83 layers\n",
        "N_BATCH = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "25c5ecbe-b93c-4985-a5fe-8b18003bcf33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25c5ecbe-b93c-4985-a5fe-8b18003bcf33",
        "outputId": "abe049c0-4caf-47d0-f842-74b910eba56c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ],
      "source": [
        "embeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": \"cpu\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e7ddedff-2d4c-4048-b72c-e08edb9da58e",
      "metadata": {
        "id": "e7ddedff-2d4c-4048-b72c-e08edb9da58e"
      },
      "outputs": [],
      "source": [
        "db = Chroma(\n",
        "        persist_directory=PERSIST_DIRECTORY,\n",
        "        embedding_function=embeddings,\n",
        "    )\n",
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c3df0e7a-5f39-4914-b900-39d3436ec76b",
      "metadata": {
        "id": "c3df0e7a-5f39-4914-b900-39d3436ec76b"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "use_history = False\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "system_prompt = \"\"\"You are a helpful assistant, you will use the provided context to answer user questions.\n",
        "Read the given context before answering questions and think step by step. If you can not answer a user question based on\n",
        "the provided context, inform the user. Do not use any other information for answering user. Provide a detailed answer to the question.\"\"\"\n",
        "\n",
        "\n",
        "# system_prompt = \"\"\"You are a helpful assistant, you will use the provided context to answer user questions.\n",
        "# Read the given context before answering questions.\n",
        "# If it contains a sequence of instructions, re-write those instructions in the following format:\n",
        "\n",
        "# Step 1 - ...\n",
        "# Step 2 - …\n",
        "# …\n",
        "# Step N - …\n",
        "\n",
        "# If the text does not contain a sequence of instructions, then only write the summary of the text.\n",
        "# If you can not answer a user question based on the provided context, inform the user.\n",
        "# Do not use any other information for answering user. Provide a detailed answer to the question.\"\"\"\n",
        "\n",
        "\n",
        "# prompt, memory = get_prompt_template(promptTemplate_type=\"llama\", history=use_history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5b88b11b-1ba3-409d-ace7-65e887d9e8d6",
      "metadata": {
        "id": "5b88b11b-1ba3-409d-ace7-65e887d9e8d6"
      },
      "outputs": [],
      "source": [
        "instruction = \"\"\"\n",
        "            Context: {context}\n",
        "            User: {question}\"\"\"\n",
        "SYSTEM_PROMPT = B_SYS + system_prompt + E_SYS\n",
        "prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
        "memory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain.llms import LlamaCpp\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    LlamaForCausalLM,\n",
        "    LlamaTokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "OmcHK6yIVd-T"
      },
      "id": "OmcHK6yIVd-T",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = hf_hub_download(\n",
        "            repo_id=MODEL_ID,\n",
        "            filename=MODEL_BASENAME,\n",
        "            resume_download=True,\n",
        "            cache_dir=MODELS_PATH,\n",
        "        )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "41384b5513b340f8802f50c025b92825",
            "e708affcac6f4b3eb372e70dd7c99879",
            "5abc3467a24449058e52f0ff8c8472de",
            "352134394aa446129d9ba29e06c160af",
            "3ab4d94f9792415485396b7137296608",
            "841174c277e9438fa827eee1afa575cc",
            "650e3d4084b146aca2605cac172ce4cb",
            "5e088071194d438d8ee25c18e25f1cb1",
            "c9efe5e4197b4c778bdfc5c18430e694",
            "21f5912b1c6947ffbf68d6ddfa894ba0",
            "34d6762056544310a034f1cf7f9cfa77"
          ]
        },
        "id": "x8kpVce9Vmla",
        "outputId": "fc2c596f-2c10-4188-9e79-fa89191ea77f"
      },
      "id": "x8kpVce9Vmla",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)-7b-chat.Q4_K_M.gguf:   0%|          | 0.00/4.08G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41384b5513b340f8802f50c025b92825"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OI2cORoXF2s",
        "outputId": "c3670105-351e-4c26-9ee0-58b7de5999fc"
      },
      "id": "3OI2cORoXF2s",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.11.tar.gz (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.11-cp310-cp310-manylinux_2_35_x86_64.whl size=1023486 sha256=b4568484b16501e68438bd33063aa054f9fd2be22462b1389b846963ce1209a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/42/77/a3ab0d02700427ea364de5797786c0272779dce795f62c3bc2\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dsVI9DeGwwtu"
      },
      "id": "dsVI9DeGwwtu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kwargs = {\n",
        "            \"model_path\": model_path,\n",
        "            \"n_ctx\": CONTEXT_WINDOW_SIZE,\n",
        "            \"max_tokens\": MAX_NEW_TOKENS,\n",
        "            \"n_batch\": N_BATCH,  # set this based on your GPU & CPU RAM\n",
        "        }\n",
        "llm = LlamaCpp(**kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrIuupzEV9mz",
        "outputId": "9fe70a17-c54f-41e2-db87-d74e3a7d2dc8"
      },
      "id": "rrIuupzEV9mz",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "aaeb3833-a6bd-4b93-86d8-10eeccc1a457",
      "metadata": {
        "id": "aaeb3833-a6bd-4b93-86d8-10eeccc1a457"
      },
      "outputs": [],
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\",  # try other chains types as well. refine, map_reduce, map_rerank\n",
        "            retriever=retriever,\n",
        "            return_source_documents=True,  # verbose=True,\n",
        "            callbacks=callback_manager,\n",
        "            chain_type_kwargs={\n",
        "                \"prompt\": prompt,\n",
        "            },\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5f55e360-3d34-4139-818a-75fab34c7f24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f55e360-3d34-4139-818a-75fab34c7f24",
        "outputId": "bbdb0d51-32e9-4e91-f63c-9b9def4c82c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Great, thank you for your question! In the context of the Standard Operating Procedure you provided, when there is an order trend low scenario, the following steps should be taken:\n",
            "1. Confirm Low Order Volume: Verify that there is indeed a decrease in order volume compared to previous periods. (Step 1 of the procedure)\n",
            "2. Review Historical Data: Analyze historical order data to identify any patterns, seasonality, or anomalies that might explain the decrease. (Step 2 of the procedure)\n",
            "3. Examine Production Services: Review the status of critical microservices and components responsible for order processing, payment, and inventory management. (Step 3 of the procedure)\n",
            "4. Performance Metrics: Analyze CPU usage, memory consumption, network activity, and response times for each service. (Step 4 of the procedure)\n",
            "5. Issue Mitigation: Implement corrective actions based on validated hypotheses. (Step 6 of the procedure)\n",
            "6. Document Findings: Maintain detailed records of findings, actions taken, and outcomes for future reference. (Step 7 of the procedure)\n",
            "By following these steps, the Amazon Operations Team can identify and address any production service issues that might be affecting order processing, thus resolving the low order volume scenario.\n"
          ]
        }
      ],
      "source": [
        "# quantized model finetuned\n",
        "query = \"What steps to do when there is a order trend low scenerio ?\"\n",
        "res = qa(query)\n",
        "answer, docs = res[\"result\"], res[\"source_documents\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "40f79c13-3818-40dd-a381-7889ea8f6dc3",
      "metadata": {
        "id": "40f79c13-3818-40dd-a381-7889ea8f6dc3"
      },
      "outputs": [],
      "source": [
        "from typing import Sequence, Optional\n",
        "from langchain.prompts import (\n",
        "    PromptTemplate,\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.llms import OpenAI\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "\n",
        "class Task(BaseModel):\n",
        "    task_name: str\n",
        "\n",
        "class Tasks(BaseModel):\n",
        "    \"\"\"Identifying information about all tasks in a text.\"\"\"\n",
        "    people: Sequence[Task]\n",
        "\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Tasks)\n",
        "\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "\n",
        "\n",
        "instruction = \"\"\"\\\n",
        "For the following context, extract the following information:\n",
        "\n",
        "task_name: Extract any automation task name that can be used for the user query and output them as json in following format.\n",
        "json format ```\n",
        "{{{{\n",
        "    \"task_name\": string \\ name of the automation task\n",
        "}}}}\n",
        "context: {context}\n",
        "User: {question}\n",
        "\"\"\"\n",
        "# SYSTEM_PROMPT = B_SYS + system_prompt + E_SYS\n",
        "# prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "prompt_template = B_INST + instruction + E_INST\n",
        "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
        "memory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "a2463a97-be51-46a7-99db-f335363ad0b8",
      "metadata": {
        "id": "a2463a97-be51-46a7-99db-f335363ad0b8"
      },
      "outputs": [],
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\",  # try other chains types as well. refine, map_reduce, map_rerank\n",
        "            retriever=retriever,\n",
        "            return_source_documents=True,  # verbose=True,\n",
        "            callbacks=callback_manager,\n",
        "            chain_type_kwargs={\n",
        "                \"prompt\": prompt,\n",
        "            },\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "6a95b751-ec3e-4590-884c-87c5cf006da6",
      "metadata": {
        "id": "6a95b751-ec3e-4590-884c-87c5cf006da6",
        "outputId": "0a13acce-917c-4b7c-be3a-2cfe6b49fad2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Based on the provided context, the following information can be extracted:\n",
            "Task Name:\n",
            "* Low Order Sanity AO\n",
            "* Validate External API Response Time\n",
            "* Load Balancing and Scaling\n",
            "* Security and DDoS\n",
            "\n",
            "Context:\n",
            "* OPS Automation Tasks\n",
            "* Objective: Different automation tasks that the ops team can use.\n",
            "\n",
            "Please find the extracted information in the format of a JSON object below:\n",
            "{{\n",
            "    \"task_name\": [\n",
            "        \"Low Order Sanity AO\",\n",
            "        \"Validate External API Response Time\",\n",
            "        \"Load Balancing and Scaling\",\n",
            "        \"Security and DDoS\"\n",
            "\n",
            "    ]\n",
            "}}\n"
          ]
        }
      ],
      "source": [
        "query = \"Which automation task should we run for low order trend scenerio\"\n",
        "res = qa(query)\n",
        "answer, docs = res[\"result\"], res[\"source_documents\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs = retriever.invoke(\"Which automation task should we run for low order trend scenerio\")\n",
        "print(retrieved_docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcxFzMYCuwVY",
        "outputId": "bc3e4c2f-1281-4628-99e8-6cbc10e9000c"
      },
      "id": "ZcxFzMYCuwVY",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OPS Automation Tasks :\n",
            "\n",
            "Objective: Different automation tasks ops team can use.\n",
            "\n",
            "Low Order Sanity AO:\n",
            "\n",
            "\t\tName: Low Order Sanity AO\n",
            "\n",
            "\t\tDescription: This task involves running an automation pipeline to check if both EIP (Enterprise Integration Patterns) and FRP (Functional Requirement Parameters) flows are passed for a low order trend scenario.\n",
            "\n",
            "\t\tRepository: The automation pipeline for this task is configured in the 'APP1-ops-synthetic-monitoring' repository.\n",
            "\n",
            "\t\tSchedule: The schedule for this automation pipeline is configured within the 'APP1-ops-synthetic-monitoring' repository.\n",
            "\n",
            "\t\tValidate External API Response Time:\n",
            "\n",
            "\t\tName: Validate External API Response Time\n",
            "\n",
            "\t\tDescription: This task involves running an automation pipeline to check if the response time or failure rate is above a threshold value for a low order trend scenario, likely involving external API calls.\n",
            "\n",
            "\t\tRepository: The automation pipeline for this task is configured in the 'APP1-ops-validator-pipelines' repository.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import create_extraction_chain\n",
        "\n",
        "# Schema\n",
        "schema = {\n",
        "    \"properties\": {\n",
        "        \"name\": {\"type\": \"string\"},\n",
        "    },\n",
        "    \"required\": [\"name\"],\n",
        "}\n",
        "\n",
        "# Input\n",
        "inp = retrieved_docs[0].page_content\n",
        "\n",
        "# Run chain\n",
        "chain = create_extraction_chain(schema, llm)\n",
        "chain.run(inp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "B9W7EXvJvGl8",
        "outputId": "147a49e6-cdcb-4921-c73e-c1741cfc6475"
      },
      "id": "B9W7EXvJvGl8",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-29ce06e1fdf0>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Run chain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_extraction_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             outputs = (\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;34m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    466\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             ]\n\u001b[0;32m--> 598\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    599\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             output = (\n\u001b[0;32m--> 491\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    492\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             text = (\n\u001b[0;32m--> 977\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/llamacpp.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;31m# and return the combined strings from the first choices's text:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mcombined_text_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             for chunk in self._stream(\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             ):\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/llamacpp.py\u001b[0m in \u001b[0;36m_stream\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \"\"\"\n\u001b[1;32m    302\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mlogprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logprobs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Llama.__call__() got an unexpected keyword argument 'functions'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser.parse(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "_2lWWGVplBLB",
        "outputId": "0801dff8-47ee-437f-f849-db29dd6f738a"
      },
      "id": "_2lWWGVplBLB",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutputParserException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mjson_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mjson_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mkw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parse_constant'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-79d4cb9894b7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Failed to parse {name} from completion {text}. Got: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOutputParserException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_format_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutputParserException\u001b[0m: Failed to parse Tasks from completion   Based on the provided context, the following information can be extracted:\nTask Name:\n* Low Order Sanity AO\n* Validate External API Response Time\n* Load Balancing and Scaling\n* Security and DDoS\n\nContext:\n* OPS Automation Tasks\n* Objective: Different automation tasks that the ops team can use.\n\nPlease find the extracted information in the format of a JSON object below:\n{{\n    \"task_name\": [\n        \"Low Order Sanity AO\",\n        \"Validate External API Response Time\",\n        \"Load Balancing and Scaling\",\n        \"Security and DDoS\"\n\n    ]\n}}. Got: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index"
      ],
      "metadata": {
        "id": "I1YzwreGwzMu"
      },
      "id": "I1YzwreGwzMu",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.program import (\n",
        "    BasePydanticProgram,\n",
        "    DFFullProgram,\n",
        "    DataFrame,\n",
        "    DataFrameRowsOnly,\n",
        ")\n",
        "\n",
        "program = BasePydanticProgram(\n",
        "    output_cls=DataFrame,\n",
        "    llm=llm,\n",
        "    prompt_template_str=(\n",
        "        \"Please extract the following query into a structured data according\"\n",
        "        \" to: {input_str}.Please extract both the set of column names and a\"\n",
        "        \" set of rows.\"\n",
        "    ),\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "response_obj = program(\n",
        "    input_str=\"\"\"My name is John and I am 25 years old. I live in\n",
        "        New York and I like to play basketball. His name is\n",
        "        Mike and he is 30 years old. He lives in San Francisco\n",
        "        and he likes to play baseball. Sarah is 20 years old\n",
        "        and she lives in Los Angeles. She likes to play tennis.\n",
        "        Her name is Mary and she is 35 years old.\n",
        "        She lives in Chicago.\"\"\"\n",
        ")\n",
        "response_obj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "Z0LlTonGxIgQ",
        "outputId": "1c2f4f80-01b4-4f34-e7f3-796deaf2f8d6"
      },
      "id": "Z0LlTonGxIgQ",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-0443f8e72ee6>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m program = BasePydanticProgram(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0moutput_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BasePydanticProgram() takes no arguments"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f97a6e2-5fa8-48e7-8630-1dbdfa4f929d",
      "metadata": {
        "id": "5f97a6e2-5fa8-48e7-8630-1dbdfa4f929d",
        "outputId": "4f2f129f-5ebf-4371-c81a-4fce83cd8775"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Based on the provided context and user query, the following automation tasks can be identified as relevant for the low order trend scenario:\n",
            "JSON Output:\n",
            "{\n",
            "\"task_name\": \"Low Order Sanity AO\",\n",
            "\"value\": \"APP1-ops-synthetic-monitoring\"\n",
            "}\n",
            "{\n",
            "\"task_name\": \"Validate External API Response time\",\n",
            "\"value\": \"APP1-ops-validator-pipelines\"\n",
            "}\n",
            "These tasks are identified based on the provided user query, which mentions the scenario of low order trend. The automation tasks listed above are relevant for this scenario, as they are designed to monitor and validate the performance of external APIs and flows in the APP1 environment. By running these tasks, the OPS team can identify any issues or anomalies that may be impacting the order trend and take corrective action accordingly.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time = 58653.81 ms\n",
            "llama_print_timings:      sample time =   223.70 ms /   194 runs   (    1.15 ms per token,   867.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =  7368.94 ms /    60 tokens (  122.82 ms per token,     8.14 tokens per second)\n",
            "llama_print_timings:        eval time = 31826.27 ms /   193 runs   (  164.90 ms per token,     6.06 tokens per second)\n",
            "llama_print_timings:       total time = 40084.63 ms\n"
          ]
        }
      ],
      "source": [
        "query = \"Find any automation that can be used for query : 'What are the steps for low order trend scenerio'\"\n",
        "res = qa(query)\n",
        "answer, docs = res[\"result\"], res[\"source_documents\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7abc137f-48f4-49c1-adfe-020c529f9937",
      "metadata": {
        "id": "7abc137f-48f4-49c1-adfe-020c529f9937",
        "outputId": "39fc8672-da56-44ba-8c95-7c11733ca401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Based on the provided context, the following are the task names that can be used for the user query:\n",
            "* \"Low order Sanity AO\"\n",
            "* \"Validate External API Response time\"\n",
            "\n",
            "The output in JSON format is:\n",
            "{\n",
            "\"task_name\": [\"Low order Sanity AO\", \"Validate External API Response time\"]\n",
            "}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time = 58601.99 ms\n",
            "llama_print_timings:      sample time =    90.51 ms /    80 runs   (    1.13 ms per token,   883.85 tokens per second)\n",
            "llama_print_timings: prompt eval time = 117849.35 ms /  1000 tokens (  117.85 ms per token,     8.49 tokens per second)\n",
            "llama_print_timings:        eval time = 13006.12 ms /    79 runs   (  164.63 ms per token,     6.07 tokens per second)\n",
            "llama_print_timings:       total time = 131204.30 ms\n"
          ]
        }
      ],
      "source": [
        "query = \"Find any automation that can be used for query : 'What are the steps for low order trend scenerio'\"\n",
        "res = qa(query)\n",
        "answer, docs = res[\"result\"], res[\"source_documents\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b07e207f-6f22-4f13-b5bd-db0a5bf10407",
      "metadata": {
        "id": "b07e207f-6f22-4f13-b5bd-db0a5bf10407",
        "outputId": "cf286c61-fb70-4c75-d11a-722dffb12b40"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'LlamaCpp' object has no attribute '_model'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m B_INST \u001b[38;5;241m+\u001b[39m instruction \u001b[38;5;241m+\u001b[39m E_INST\n\u001b[1;32m     36\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m], template\u001b[38;5;241m=\u001b[39mprompt_template)\n\u001b[0;32m---> 38\u001b[0m logits_processors \u001b[38;5;241m=\u001b[39m LogitsProcessorList([build_llamacpp_logits_processor(\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m, JsonSchemaParser(AnswerFormat\u001b[38;5;241m.\u001b[39mschema()))])\n\u001b[1;32m     40\u001b[0m qa \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[1;32m     41\u001b[0m             llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     42\u001b[0m             chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# try other chains types as well. refine, map_reduce, map_rerank\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m             },\n\u001b[1;32m     50\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LlamaCpp' object has no attribute '_model'"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "from typing import Optional\n",
        "from llama_cpp import LogitsProcessorList\n",
        "from lmformatenforcer import CharacterLevelParser, JsonSchemaParser\n",
        "from lmformatenforcer.integrations.llamacpp import build_llamacpp_logits_processor\n",
        "\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\\n",
        "\"\"\"\n",
        "\n",
        "instruction = \"\"\"\\\n",
        "For the following context and user query, extract the following information:\n",
        "\n",
        "context: {context}\n",
        "User: {question}\n",
        "\n",
        "task_name: Extract any automation task name that can be used for the user query and output them as a comma separated Python list.\n",
        "\n",
        "You MUST answer using the following json schema:\n",
        "task_name : str\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_prompt(message: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
        "    return f'<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{message} [/INST]'\n",
        "\n",
        "class AnswerFormat(BaseModel):\n",
        "    task_name: str\n",
        "\n",
        "# question = 'Please give me information about Michael Jordan. You MUST answer using the following json schema: '\n",
        "# question_with_schema = f'{question}{AnswerFormat.schema_json()}'\n",
        "schema = AnswerFormat.schema_json()\n",
        "prompt = get_prompt(question_with_schema)\n",
        "prompt_template = B_INST + instruction + E_INST\n",
        "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
        "\n",
        "logits_processors = LogitsProcessorList([build_llamacpp_logits_processor(llm._model, JsonSchemaParser(AnswerFormat.schema()))])\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\",  # try other chains types as well. refine, map_reduce, map_rerank\n",
        "            retriever=retriever,\n",
        "            return_source_documents=True,  # verbose=True,\n",
        "            callbacks=callback_manager,\n",
        "            chain_type_kwargs={\n",
        "                \"prompt\": prompt,\n",
        "                \"logits_processor\": logits_processors,\n",
        "            },\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d179c65b-4d0b-4415-8919-f002b8fffc21",
      "metadata": {
        "id": "d179c65b-4d0b-4415-8919-f002b8fffc21"
      },
      "outputs": [],
      "source": [
        "output_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "task_name: Extract any automation task name that can be used for the user query and output them as a comma separated Python list.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "task_name\n",
        "\n",
        "text: {text}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d52f96-e173-40d0-bdda-bbb2f63a6005",
      "metadata": {
        "id": "52d52f96-e173-40d0-bdda-bbb2f63a6005",
        "outputId": "d85512b6-7c23-4514-ac63-0733aa1d7b41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_variables=['text'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='For the following text, extract the following information:\\n\\ntask_name: Extract any automation task name that can be used for the user query and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ntask_name\\n\\ntext: {text}\\n'))]\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(output_template)\n",
        "print(prompt_template)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "41384b5513b340f8802f50c025b92825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e708affcac6f4b3eb372e70dd7c99879",
              "IPY_MODEL_5abc3467a24449058e52f0ff8c8472de",
              "IPY_MODEL_352134394aa446129d9ba29e06c160af"
            ],
            "layout": "IPY_MODEL_3ab4d94f9792415485396b7137296608"
          }
        },
        "e708affcac6f4b3eb372e70dd7c99879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_841174c277e9438fa827eee1afa575cc",
            "placeholder": "​",
            "style": "IPY_MODEL_650e3d4084b146aca2605cac172ce4cb",
            "value": "Downloading (…)-7b-chat.Q4_K_M.gguf: 100%"
          }
        },
        "5abc3467a24449058e52f0ff8c8472de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e088071194d438d8ee25c18e25f1cb1",
            "max": 4081004224,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9efe5e4197b4c778bdfc5c18430e694",
            "value": 4081004224
          }
        },
        "352134394aa446129d9ba29e06c160af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21f5912b1c6947ffbf68d6ddfa894ba0",
            "placeholder": "​",
            "style": "IPY_MODEL_34d6762056544310a034f1cf7f9cfa77",
            "value": " 4.08G/4.08G [00:36&lt;00:00, 246MB/s]"
          }
        },
        "3ab4d94f9792415485396b7137296608": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "841174c277e9438fa827eee1afa575cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "650e3d4084b146aca2605cac172ce4cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e088071194d438d8ee25c18e25f1cb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9efe5e4197b4c778bdfc5c18430e694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21f5912b1c6947ffbf68d6ddfa894ba0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34d6762056544310a034f1cf7f9cfa77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}