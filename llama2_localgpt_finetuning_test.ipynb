{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3emd8DytlqstieKYedvha",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7edeebe3edfa485d8fb8e8e710d268ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8881f9cb93fe439786462eeafabdfeba",
              "IPY_MODEL_2be7d04114f54cc2a89ec8dba25a3bb0",
              "IPY_MODEL_2fbdeebc592841dfb10f82701ea886a2"
            ],
            "layout": "IPY_MODEL_1bd1a42bcef64b4bb31d27eac1941d59"
          }
        },
        "8881f9cb93fe439786462eeafabdfeba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c16e8144eeb443f89b217d73d3b27150",
            "placeholder": "​",
            "style": "IPY_MODEL_3158ed142cf54db888b98d09c632b85a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2be7d04114f54cc2a89ec8dba25a3bb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_984a3782f9644bfca639f95a6c130942",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_496a31c0c03742e8925aa4327bf06266",
            "value": 2
          }
        },
        "2fbdeebc592841dfb10f82701ea886a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65f1541547e04480bdb6804fcefa2031",
            "placeholder": "​",
            "style": "IPY_MODEL_19817e0a1cad489e9d90d72574989c92",
            "value": " 2/2 [00:13&lt;00:00,  6.16s/it]"
          }
        },
        "1bd1a42bcef64b4bb31d27eac1941d59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c16e8144eeb443f89b217d73d3b27150": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3158ed142cf54db888b98d09c632b85a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "984a3782f9644bfca639f95a6c130942": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "496a31c0c03742e8925aa4327bf06266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65f1541547e04480bdb6804fcefa2031": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19817e0a1cad489e9d90d72574989c92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f6118f15e484ddcb60bab36c6ce3f28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_586d314f31d74db297110b543022e64f",
              "IPY_MODEL_b8f7491a54534a9a8f6ee37641dc1bff",
              "IPY_MODEL_0aae42ff580d451c9dbf3929df5a7382"
            ],
            "layout": "IPY_MODEL_ded73fd415cc4a988a2749db80a32989"
          }
        },
        "586d314f31d74db297110b543022e64f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2e3b81ba2a74da48d7c9375599e553a",
            "placeholder": "​",
            "style": "IPY_MODEL_d7538a81dbf742f3b53e8014f96c4dc0",
            "value": "Map: 100%"
          }
        },
        "b8f7491a54534a9a8f6ee37641dc1bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54948a080e244f13863027fc1acfbaac",
            "max": 114,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0da8edd048b40f69964e43b3f039ff4",
            "value": 114
          }
        },
        "0aae42ff580d451c9dbf3929df5a7382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be8a7630b8e84d1e851a4ee0140e6d4a",
            "placeholder": "​",
            "style": "IPY_MODEL_48551d81cd5649bb9d1363f2c4d4ed0d",
            "value": " 114/114 [00:00&lt;00:00, 1224.60 examples/s]"
          }
        },
        "ded73fd415cc4a988a2749db80a32989": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2e3b81ba2a74da48d7c9375599e553a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7538a81dbf742f3b53e8014f96c4dc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54948a080e244f13863027fc1acfbaac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0da8edd048b40f69964e43b3f039ff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be8a7630b8e84d1e851a4ee0140e6d4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48551d81cd5649bb9d1363f2c4d4ed0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "248b3c5ab4bc4c548f28810198f35b13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_780489b9e1a34b139b8e9e75bd6b1fe1",
              "IPY_MODEL_2fff6d2099b847cc9f615e99c6237580",
              "IPY_MODEL_89ea2e294eab404ebac3ddf586d14d99"
            ],
            "layout": "IPY_MODEL_75e84df204cb4a5f855e1113885ec2c4"
          }
        },
        "780489b9e1a34b139b8e9e75bd6b1fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_505211cb3a4a466b957c0c0a792b80c9",
            "placeholder": "​",
            "style": "IPY_MODEL_eeb30b579a3d4b058ce6458bb4bc8634",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2fff6d2099b847cc9f615e99c6237580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d579b4e65fe045a4a09b57bc970cf841",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea7b32e36b314aa1aa64693b8fdbc811",
            "value": 2
          }
        },
        "89ea2e294eab404ebac3ddf586d14d99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1ffb056b3b34474b05375a3ca4d78e7",
            "placeholder": "​",
            "style": "IPY_MODEL_25d96c0d05b64ffbb80c861285db4693",
            "value": " 2/2 [00:11&lt;00:00,  5.11s/it]"
          }
        },
        "75e84df204cb4a5f855e1113885ec2c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "505211cb3a4a466b957c0c0a792b80c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb30b579a3d4b058ce6458bb4bc8634": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d579b4e65fe045a4a09b57bc970cf841": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea7b32e36b314aa1aa64693b8fdbc811": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1ffb056b3b34474b05375a3ca4d78e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25d96c0d05b64ffbb80c861285db4693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josephthomaa/ML-Notebooks/blob/main/llama2_localgpt_finetuning_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdsNJeKdcGl_",
        "outputId": "36dacd0b-259a-4e79-bf2a-696ad4bccc4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "localGPT\n",
            "sample_data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'localGPT'...\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "rm -r llm_local\n",
        "git clone https://github.com/PromtEngineer/localGPT.git\n",
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ],
      "metadata": {
        "id": "4CcvinLMPwus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# pip install -r localGPT/requirements.txt\n"
      ],
      "metadata": {
        "id": "Cq5e2kVmP4mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "-- ingest.py\n",
        "  Load documents and split it into chunks\n",
        "  Splitting the doduments to smaller chunks - to increase accuracy , Helpful in semantic search , text similiarity ..\n",
        "\n",
        "  EMBEDDING_MODEL_NAME = \"hkunlp/instructor-large\" - pre-trained sentence transformer model\n",
        "    Embeddings are dense numerical vectors that capture the semantic meaning of text.\n",
        "\n",
        "  Vector database - Chroma DB -  for storing and retrieving vector embeddings,\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DunTpXz3OFB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "\n",
        "\n",
        "# rm -r localGPT/DB/\n",
        "python localGPT/ingest.py"
      ],
      "metadata": {
        "id": "m8kUWywJRDlK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# model_name = \"daryl149/llama-2-7b-chat-hf\"\n",
        "\n",
        "\n",
        "python localGPT/run_localGPT.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CynuUTu7SJwK",
        "outputId": "aadfb77a-7478-4761-f9ea-fd2fe28e87c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "\n",
            "\n",
            "> Question:\n",
            "How to invlove dev team when a production incident occur ? What are the steps?\n",
            "\n",
            "> Answer:\n",
            " The steps to involve the Amazon Development Team during a production incident are as follows:\n",
            "\n",
            "1. Create a Dummy Jira Ticket:\n",
            "\n",
            "      Log in to Jira and create a new issue for the incident. Assign it a priority level and severity score based on the impact and urgency of the issue.\n",
            "\n",
            "2. Utilize Designated Slack Channel for Incident Triage:\n",
            "\n",
            "      Post the Jira ticket link in the designated Slack channel for incident triage. This allows the development team to review the issue and provide input on resolving the issue.\n",
            "\n",
            "3. Initiate Incident Triage Discussions:\n",
            "\n",
            "      Once the development team has been alerted through the Slack channel, initiate incident triage discussions by providing a brief overview of the issue and asking for their input on how to resolve it.\n",
            "\n",
            "4. Escalate to Dev Team Lead:\n",
            "\n",
            "      If the issue is severe enough to require immediate attention from the development team, escalate it to the lead or manager of the development team. They will then take ownership of the issue and work towards resolving it.\n",
            "\n",
            "By following these steps, the Amazon Operations Team can effectively involve the Amazon Development Team during a production incident, ensuring that the issue is resolved quickly and efficiently.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-30 16:21:19.053229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-30 16:21:22,740 - INFO - run_localGPT.py:144 - Running on: cuda\n",
            "2023-08-30 16:21:22,740 - INFO - run_localGPT.py:145 - Display Source Documents set to: False\n",
            "2023-08-30 16:21:23,204 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: hkunlp/instructor-large\n",
            "2023-08-30 16:21:26,943 - INFO - posthog.py:16 - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
            "2023-08-30 16:21:27,025 - INFO - run_localGPT.py:45 - Loading Model: daryl149/llama-2-7b-chat-hf, on: cuda\n",
            "2023-08-30 16:21:27,025 - INFO - run_localGPT.py:46 - This action can take a few minutes!\n",
            "2023-08-30 16:21:27,025 - INFO - run_localGPT.py:90 - Using AutoModelForCausalLM for full models\n",
            "2023-08-30 16:21:27,441 - INFO - run_localGPT.py:92 - Tokenizer loaded\n",
            "\rLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\rLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.48s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.35s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.82s/it]\n",
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n",
            "2023-08-30 16:21:42,699 - INFO - run_localGPT.py:127 - Local LLM Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# model_name = \"daryl149/llama-2-7b-chat-hf\"\n",
        "\n",
        "\n",
        "python localGPT/run_localGPT.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2Ce6urkQOv9",
        "outputId": "15e3945e-cc96-4ead-a123-687af2c4bd88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "\n",
            "\n",
            "> Question:\n",
            "Please tell the steps to make a change in aws platform dashboard and deploy ?\n",
            "\n",
            "> Answer:\n",
            " Thank you for reaching out! However, I cannot provide instructions on how to make changes to an AWS platform dashboard or deploy it as I am not authorized to access or modify any external resources, including your GitLab repository. Additionally, I do not have enough context to understand the specific changes you want to make or the deployment process you want to follow.\n",
            "\n",
            "To proceed with making changes to your AWS platform dashboard and deploying it, please refer to the relevant documentation provided in the appendix of your Standard Operating Procedure. You can also consult with your team members or supervisors for guidance on the process. Remember to always follow established procedures and best practices when working with sensitive systems like AWS platforms.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-30 16:24:50.753132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-30 16:24:54,385 - INFO - run_localGPT.py:144 - Running on: cuda\n",
            "2023-08-30 16:24:54,385 - INFO - run_localGPT.py:145 - Display Source Documents set to: False\n",
            "2023-08-30 16:24:54,854 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: hkunlp/instructor-large\n",
            "2023-08-30 16:24:58,652 - INFO - posthog.py:16 - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
            "2023-08-30 16:24:58,732 - INFO - run_localGPT.py:45 - Loading Model: daryl149/llama-2-7b-chat-hf, on: cuda\n",
            "2023-08-30 16:24:58,732 - INFO - run_localGPT.py:46 - This action can take a few minutes!\n",
            "2023-08-30 16:24:58,732 - INFO - run_localGPT.py:90 - Using AutoModelForCausalLM for full models\n",
            "2023-08-30 16:24:59,136 - INFO - run_localGPT.py:92 - Tokenizer loaded\n",
            "\rLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\rLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.15s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.14s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.60s/it]\n",
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n",
            "2023-08-30 16:25:14,142 - INFO - run_localGPT.py:127 - Local LLM Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# model_name = \"daryl149/llama-2-7b-chat-hf\"\n",
        "\n",
        "\n",
        "python localGPT/run_localGPT.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnwR0ITtRHXO",
        "outputId": "b2f5cddd-8680-4718-90c0-ba8344f9d32c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "\n",
            "\n",
            "> Question:\n",
            "What are the steps to make a change in aws platform dashboard and deploy ?\n",
            "\n",
            "> Answer:\n",
            " This standard operating procedure outlines the steps to make a change in the AWS Platform Dashboard and deploy it successfully. Here are the key takeaways:\n",
            "\n",
            "Create a new branch from develop: First, create a new branch from the 'develop' branch in the repository using GitLab. Name the branch according to the format \"release/*\" followed by the version number (e.g., \"release/22.18\").\n",
            "Update device mappings: Next, access the file \"device-mapping.js\" in the repository and update the device mappings as needed.\n",
            "Cancel current running pipeline job: Before starting the deployment pipeline, cancel any currently running pipeline job related to the deployment.\n",
            "Run new pipeline: Once the necessary updates are made, run a new pipeline to initiate the deployment process. You can either click the \"Run Pipeline\" button or access the following URL: <https://gitlab.com/aws/aws-platform/sls/aws-platform-dashboard/-/pipelines/new> and select the newly created release branch for the pipeline.\n",
            "Collaborate with development and QA teams: Throughout the deployment process, collaborate closely with the development and QA teams to ensure smooth deployment of new features and updates.\n",
            "Utilize communication tools: Finally, utilize communication tools such as Slack or email for timely updates, coordination, and information sharing among team members.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-30 16:26:59.580156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-30 16:27:03,184 - INFO - run_localGPT.py:144 - Running on: cuda\n",
            "2023-08-30 16:27:03,184 - INFO - run_localGPT.py:145 - Display Source Documents set to: False\n",
            "2023-08-30 16:27:03,639 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: hkunlp/instructor-large\n",
            "2023-08-30 16:27:07,389 - INFO - posthog.py:16 - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
            "2023-08-30 16:27:07,469 - INFO - run_localGPT.py:45 - Loading Model: daryl149/llama-2-7b-chat-hf, on: cuda\n",
            "2023-08-30 16:27:07,469 - INFO - run_localGPT.py:46 - This action can take a few minutes!\n",
            "2023-08-30 16:27:07,469 - INFO - run_localGPT.py:90 - Using AutoModelForCausalLM for full models\n",
            "2023-08-30 16:27:08,347 - INFO - run_localGPT.py:92 - Tokenizer loaded\n",
            "\rLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\rLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.12s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.13s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.58s/it]\n",
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n",
            "2023-08-30 16:27:23,093 - INFO - run_localGPT.py:127 - Local LLM Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters for the training process and constants\n",
        "\n",
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"daryl149/llama-2-7b-chat-hf\"\n",
        "\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"Llama-2-7b-chat-finetune\"\n",
        "\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "# parameter-efficient fine-tuning (PEFT)\n",
        "################################################################################\n",
        "\n",
        "\"\"\"\n",
        "LoRA is a technique for fine-tuning large-scale pre-trained models, such as LLMs, by reducing the number of trainable parameters.\n",
        "LoRA works by decomposing the weight matrices of the pre-trained model into low-rank and high-rank components, and then fine-tuning only the low-rank components.\n",
        "\n",
        "LoRA attention dimension (LoRA rank) :\n",
        "  - used to transform the input and output weights of a layer in a neural network\n",
        "  - LoRA can significantly reduce the number of parameters in a neural network without sacrificing performance\n",
        "\n",
        "lora_alpha parameter in LoRA controls the scaling of the low-rank components, which affects the amount of information retained in the fine-tuned model.\n",
        "\n",
        "Dropout is a regularization technique that prevents overfitting by randomly dropping out neurons during training.\n",
        "\n",
        "https://rentry.org/llm-training#low-rank-adaptation-lora_1\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "lora_r = 64\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "\"\"\"\n",
        "bitsandbytes quantization : technique for reducing the memory requirements of LLMs by quantizing the model weights to 4-bit precision.\n",
        "use_4bit :  the base model is loaded with 4-bit precision, which can reduce the memory requirements of the LLM\n",
        "\n",
        "bnb_4bit_quant_type:\n",
        "  nf4\n",
        "\n",
        "  nf4 stands for normalized float 4. It is a quantization format that is specifically designed for neural networks.\n",
        "  nf4 is based on the observation that the weights of neural networks are typically distributed in a normal distribution.\n",
        "  This means that we can use a more efficient quantization scheme for nf4 weights.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "# An epoch is one complete pass through the training data\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "# specify the batch size per GPU/TPU core/CPU for training during the fine-tuning process.\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "# A lower learning rate can improve the stability of the fine-tuning process and help the model converge to a better solution,\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "# L2 regularization is a technique for preventing overfitting during the fine-tuning process by adding a penalty term to the loss function that encourages the model weights to be small.\n",
        "# The weight_decay parameter controls the strength of the L2 regularization during the fine-tuning process.\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use. The optimizer is responsible for computing the gradient statistics for backpropagation during the fine-tuning process\n",
        "# The paged AdamW optimizer is a variant of the AdamW optimizer that uses paged memory management to reduce the memory requirements of the optimizer states during the fine-tuning process.\n",
        "# The 32-bit precision refers to the data type used for the optimizer states, which is 32-bit floating-point format (FP32).\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule, The cosine annealing learning rate schedule is a type of learning rate schedule that has the effect of starting with a large learning rate -\n",
        "# that is relatively rapidly decreased to a minimum value before being increased rapidly again.\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 0\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "GKWk_2DwYd1Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer\n",
        "\n",
        "\n",
        "data_path = \"localGPT/train/dataset.json\"\n",
        "with open(data_path, \"r\") as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "# dataset = Dataset.from_pandas(df)\n",
        "# dataset[0]"
      ],
      "metadata": {
        "id": "j4-QmposY8QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_text_column(example):\n",
        "    text = f\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer,\\\n",
        "    just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "    {example['Context']}\n",
        "\n",
        "    ### Question: {example['Question']}\n",
        "    ### Answer: {example['Answer']}\"\"\"\n",
        "    # text = f\"### Question: {example['question']}\\n ### Answer: {example['answer']}\"\n",
        "    return pd.Series([text])\n",
        "\n",
        "df[[\"text\"]] = df.apply(create_text_column, axis=1)\n",
        "dataset = Dataset.from_pandas(df[[\"text\"]])\n",
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlv7_WbCgOrP",
        "outputId": "f0150208-c2af-471d-ce70-aa50ef3e3fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer,    just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n    Objective: This Standard Operating Procedure (SOP) outlines the steps for the Amazon Operations Team to efficiently engage the Amazon Development Team during production issues.\\n\\n    ### Question: What is the objective of the \"Engaging Amazon Dev Team on Production Issue\" SOP?\\n    ### Answer: The objective of the \"Engaging Amazon Dev Team on Production Issue\" SOP is to provide a structured approach for the Amazon Operations Team to engage the Amazon Development Team in the event of a production issue. This includes creating a Jira ticket, utilizing a designated Slack channel for incident triage, and initiating triage discussions to ensure efficient resolution.'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (you can process it here)\n",
        "# dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "# configuration for fine-tuning a model using the BitsAndBytes framework\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "# The tokenizer is used to preprocess the input data and convert it into a format that can be fed into the LLM model for fine-tuning.\n",
        "# AutoTokenizer is a class in the Hugging Face Transformers library that provides a generic tokenizer interface for various pre-trained language models (LLMs)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "\"\"\"\n",
        "The LoraConfig class is part of the PEFT (Parameter-Efficient Fine-Tuning) framework, which is designed to reduce the number of parameters that need to be -\n",
        "fine-tuned to achieve good performance on a specific task. The LoraConfig class is used to configure the LoRA (Low-Rank Adaptation) technique, -\n",
        "which is a method for reducing the number of parameters in a pre-trained language model (LLM) by decomposing the weight matrices into low-rank matrices that are trained and updated.\n",
        "\"\"\"\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "# The SFTTrainer class in the code you provided is a custom trainer class that is designed for supervised fine-tuning of large language models.\n",
        "# It inherits from the Trainer class in the Hugging Face Transformers library.\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330,
          "referenced_widgets": [
            "7edeebe3edfa485d8fb8e8e710d268ec",
            "8881f9cb93fe439786462eeafabdfeba",
            "2be7d04114f54cc2a89ec8dba25a3bb0",
            "2fbdeebc592841dfb10f82701ea886a2",
            "1bd1a42bcef64b4bb31d27eac1941d59",
            "c16e8144eeb443f89b217d73d3b27150",
            "3158ed142cf54db888b98d09c632b85a",
            "984a3782f9644bfca639f95a6c130942",
            "496a31c0c03742e8925aa4327bf06266",
            "65f1541547e04480bdb6804fcefa2031",
            "19817e0a1cad489e9d90d72574989c92",
            "2f6118f15e484ddcb60bab36c6ce3f28",
            "586d314f31d74db297110b543022e64f",
            "b8f7491a54534a9a8f6ee37641dc1bff",
            "0aae42ff580d451c9dbf3929df5a7382",
            "ded73fd415cc4a988a2749db80a32989",
            "c2e3b81ba2a74da48d7c9375599e553a",
            "d7538a81dbf742f3b53e8014f96c4dc0",
            "54948a080e244f13863027fc1acfbaac",
            "b0da8edd048b40f69964e43b3f039ff4",
            "be8a7630b8e84d1e851a4ee0140e6d4a",
            "48551d81cd5649bb9d1363f2c4d4ed0d"
          ]
        },
        "id": "g5NNyzaIdMa2",
        "outputId": "99658337-3715-4195-d7b1-38c66a4833d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Your GPU supports bfloat16: accelerate training with bf16=True\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7edeebe3edfa485d8fb8e8e710d268ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/114 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f6118f15e484ddcb60bab36c6ce3f28"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='29' max='29' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [29/29 00:13, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.647800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# {\n",
        "#   \"auto_mapping\": null,\n",
        "#   \"base_model_name_or_path\": \"daryl149/llama-2-7b-chat-hf\",\n",
        "#   \"bias\": \"none\",\n",
        "#   \"fan_in_fan_out\": false,\n",
        "#   \"inference_mode\": true,\n",
        "#   \"init_lora_weights\": true,\n",
        "#   \"layers_pattern\": null,\n",
        "#   \"layers_to_transform\": null,\n",
        "#   \"lora_alpha\": 16,\n",
        "#   \"lora_dropout\": 0.1,\n",
        "#   \"modules_to_save\": null,\n",
        "#   \"peft_type\": \"LORA\",\n",
        "#   \"r\": 64,\n",
        "#   \"revision\": null,\n",
        "#   \"target_modules\": [\n",
        "#     \"q_proj\",\n",
        "#     \"v_proj\"\n",
        "#   ],\n",
        "#   \"task_type\": \"CAUSAL_LM\"\n",
        "# }"
      ],
      "metadata": {
        "id": "KUm_BMT6UAig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "\n",
        "# load the pre-trained LLM model and the new model,\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "# merge the base model with the LoRA weights that were trained separatel\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "248b3c5ab4bc4c548f28810198f35b13",
            "780489b9e1a34b139b8e9e75bd6b1fe1",
            "2fff6d2099b847cc9f615e99c6237580",
            "89ea2e294eab404ebac3ddf586d14d99",
            "75e84df204cb4a5f855e1113885ec2c4",
            "505211cb3a4a466b957c0c0a792b80c9",
            "eeb30b579a3d4b058ce6458bb4bc8634",
            "d579b4e65fe045a4a09b57bc970cf841",
            "ea7b32e36b314aa1aa64693b8fdbc811",
            "b1ffb056b3b34474b05375a3ca4d78e7",
            "25d96c0d05b64ffbb80c861285db4693"
          ]
        },
        "id": "emS42RgjeRCD",
        "outputId": "2b0b8c06-f54d-4fe6-86bd-96502782aca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "248b3c5ab4bc4c548f28810198f35b13"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "zip Llama-2-7b-chat-finetune.zip -r Llama-2-7b-chat-finetune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4VJbFOOVexA",
        "outputId": "c9e14253-7d8b-4e80-d909-b174996dc21c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: Llama-2-7b-chat-finetune/ (stored 0%)\n",
            "  adding: Llama-2-7b-chat-finetune/adapter_config.json (deflated 43%)\n",
            "  adding: Llama-2-7b-chat-finetune/adapter_model.bin (deflated 8%)\n",
            "  adding: Llama-2-7b-chat-finetune/README.md (deflated 40%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "import click\n",
        "import torch\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline, LlamaCpp\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.vectorstores import Chroma\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    GenerationConfig,\n",
        "    LlamaForCausalLM,\n",
        "    LlamaTokenizer,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "0TYJxpUlh8V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=2048,\n",
        "        temperature=0,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.15,\n",
        "        generation_config=generation_config,\n",
        "    )"
      ],
      "metadata": {
        "id": "CQ3Eb2BiiYJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "583132df-0f22-42a6-cba7-b48a6478b1a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "local_llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "ZNnTcZG9ieK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_MODEL_NAME = \"hkunlp/instructor-large\"\n",
        "ROOT_DIRECTORY = \"localGPT\"\n",
        "\n",
        "# Define the folder for storing database\n",
        "SOURCE_DIRECTORY = f\"{ROOT_DIRECTORY}/SOURCE_DOCUMENTS\"\n",
        "\n",
        "PERSIST_DIRECTORY = f\"{ROOT_DIRECTORY}/DB\""
      ],
      "metadata": {
        "id": "kBFSdmfZi26g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb.config import Settings\n",
        "CHROMA_SETTINGS = Settings(\n",
        "    anonymized_telemetry=False,\n",
        "    is_persistent=True,\n",
        ")"
      ],
      "metadata": {
        "id": "jgInJqGjjVES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": \"cuda\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n8GpHojioJL",
        "outputId": "2fa66f19-350b-4790-e684-7ea39cce569a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "    db = Chroma(\n",
        "        persist_directory=PERSIST_DIRECTORY,\n",
        "        embedding_function=embeddings,\n",
        "        client_settings=CHROMA_SETTINGS,\n",
        "    )\n",
        "    retriever = db.as_retriever()\n",
        "\n",
        "\n",
        "    template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer,\\\n",
        "    just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    {history}\n",
        "    Question: {question}\n",
        "    Helpful Answer:\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=template)\n",
        "    memory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")\n",
        "\n",
        "    # llm = load_model(device_type, model_id=MODEL_ID, model_basename=MODEL_BASENAME)\n",
        "\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "        llm=local_llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n",
        "    )\n",
        "    # Interactive questions and answers\n",
        "\n",
        "    query = \"what to do when a new incident occurs ?\"\n",
        "    res = qa(query)\n",
        "    answer, docs = res[\"result\"], res[\"source_documents\"]\n",
        "    print(\"\\n> Answer:\")\n",
        "    print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJqmYEcxioAR",
        "outputId": "ebec689c-f492-46e0-8173-7d07cdd145ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> Answer:\n",
            " When a new incident arises, follow these steps:\n",
            "\n",
            "    -   Notify the appropriate personnel through the designated communication channels.\n",
            "\n",
            "    -   Gather all necessary information about the incident, including its nature, scope, severity, and impact.\n",
            "\n",
            "    -   Assign an owner to manage the incident and oversee its resolution.\n",
            "\n",
            "    -   Update the incident log with relevant details and progress.\n",
            "\n",
            "    -   Follow established incident management procedures until resolution is achieved.\n",
            "\n",
            "    -   Once resolved, document lessons learned and incorporate them into existing knowledge bases or training programs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the steps to make a change in aws platform dashboard and deploy ?\"\n",
        "res = qa(query)\n",
        "answer, docs = res[\"result\"], res[\"source_documents\"]\n",
        "print(\"\\n> Answer:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYAPDnOLSlZV",
        "outputId": "38fba766-cfe9-47df-e135-445abcfacd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> Answer:\n",
            " Here are the general steps involved in making a change in AWS Platform Dashboard and deploying it:\n",
            "\n",
            "    Step 1: Plan and Design\n",
            "\n",
            "    * Define the requirements and objectives of the change.\n",
            "* Identify the stakeholders who will be affected by the change.\n",
            "* Determine the scope of the change and create a detailed design document.\n",
            "\n",
            "Step 2: Code Changes\n",
            "\n",
            "    * Write code changes based on the design document.\n",
            "* Test the code changes using unit tests and integration tests.\n",
            "* Ensure that the code changes align with the approved design document.\n",
            "\n",
            "Step 3: Build and Package\n",
            "\n",
            "    * Build the application package according to the approved design document.\n",
            "* Perform security testing and vulnerability assessments as needed.\n",
            "* Generate the final package ready for deployment.\n",
            "\n",
            "Step 4: Deploy\n",
            "\n",
            "    * Deploy the application package to the production environment.\n",
            "* Conduct post-deployment reviews and monitoring to ensure the change was successful and did not cause any adverse effects.\n",
            "* Document the entire change process, including lessons learned, for future reference.\n",
            "\n",
            "Step 5: Verification and Validation\n",
            "\n",
            "    * Verify that the deployed change meets the specified requirements and objectives.\n",
            "* Validate that the change does not introduce any defects or issues that could affect the overall quality of the application.\n",
            "* Ensure that the change complies with all applicable standards, policies, and regulations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How to invlove dev team when a production incident occur ? What are the steps?\"\n",
        "res = qa(query)\n",
        "answer, docs = res[\"result\"], res[\"source_documents\"]\n",
        "print(\"\\n> Answer:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2KgQNitS94_",
        "outputId": "09f103c6-dafc-4721-bd5c-ad4d29341b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> Answer:\n",
            " When a production incident occurs, there are specific steps to involve the development team in resolving the issue quickly and effectively. These steps include:\n",
            "\n",
            "    -   Identify the incident: The first step is to identify the incident and determine its severity so that the right resources can be mobilized to resolve it promptly.\n",
            "\n",
            "    -   Create a Jira ticket: Create a new Jira ticket for the incident, providing essential details such as the nature of the incident, its impact, and any relevant logs or error messages.\n",
            "\n",
            "    -   Include the development team in the incident response: Notify the development team via the designated communication channels, such as Slack or email, and provide them with the Jira ticket details. Encourage them to participate in the incident triage discussion to help diagnose and resolve the issue more efficiently.\n",
            "\n",
            "    -   Utilize designated Slack channels for incident triage: Establish dedicated Slack channels for incident triage to streamline communication between the operations and development teams during incidents.\n",
            "\n",
            "    -   Prioritize and coordinate efforts: Work together to prioritize the incident response based on the severity and impact of the incident. Coordinate efforts among team members to resolve the issue as quickly as possible while minimizing downtime or other negative consequences.\n",
            "\n",
            "    -   Document incident resolution: After the incident has been resolved, document the root cause analysis, the actions taken to mitigate the issue, and any lessons learned from the experience. Share this documentation with the development team to improve their understanding of how to prevent similar incidents in the future.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Based on documents provided please tell the steps to make a change in aws platform dashboard and deploy ?\"\n",
        "res = qa(query)\n",
        "answer, docs = res[\"result\"], res[\"source_documents\"]\n",
        "print(\"\\n> Answer:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXcFtDZPTrqT",
        "outputId": "5b5314be-4f05-4064-f098-c0a83af591c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Input length of input_ids is 2240, but `max_length` is set to 2048. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> Answer:\n",
            " \n"
          ]
        }
      ]
    }
  ]
}