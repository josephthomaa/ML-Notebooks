{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josephthomaa/ML-Notebooks/blob/main/Llama_finetuned_quantized_JT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "git clone https://github.com/PromtEngineer/localGPT.git\n",
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZM8cX3NK9IS",
        "outputId": "66482eee-7ccc-4e29-8ad3-24b6bd55c704"
      },
      "id": "5ZM8cX3NK9IS",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "localGPT\n",
            "sample_data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'localGPT'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "pip install -r localGPT/requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WSxsJ3-rLK8G",
        "outputId": "c2ce4d3c-0e64-4b16-c32a-7857063d1f53"
      },
      "id": "WSxsJ3-rLK8G",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ignoring protobuf: markers 'sys_platform == \"darwin\" and platform_machine != \"arm64\"' don't match your environment\n",
            "Ignoring protobuf: markers 'sys_platform == \"darwin\" and platform_machine == \"arm64\"' don't match your environment\n",
            "Ignoring bitsandbytes-windows: markers 'sys_platform == \"win32\"' don't match your environment\n",
            "Collecting langchain==0.0.267 (from -r localGPT/requirements.txt (line 2))\n",
            "  Downloading langchain-0.0.267-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb==0.4.6 (from -r localGPT/requirements.txt (line 3))\n",
            "  Downloading chromadb-0.4.6-py3-none-any.whl (405 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.5/405.5 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20221105 (from -r localGPT/requirements.txt (line 4))\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting InstructorEmbedding (from -r localGPT/requirements.txt (line 5))\n",
            "  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\n",
            "Collecting sentence-transformers (from -r localGPT/requirements.txt (line 6))\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faiss-cpu (from -r localGPT/requirements.txt (line 7))\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface_hub (from -r localGPT/requirements.txt (line 8))\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers (from -r localGPT/requirements.txt (line 9))\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf==3.20.2 (from -r localGPT/requirements.txt (line 10))\n",
            "  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting auto-gptq==0.2.2 (from -r localGPT/requirements.txt (line 13))\n",
            "  Downloading auto_gptq-0.2.2.tar.gz (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docx2txt (from -r localGPT/requirements.txt (line 14))\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unstructured (from -r localGPT/requirements.txt (line 15))\n",
            "  Downloading unstructured-0.10.27-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3==1.26.6 (from -r localGPT/requirements.txt (line 19))\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate (from -r localGPT/requirements.txt (line 20))\n",
            "  Downloading accelerate-0.24.0-py3-none-any.whl (260 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes (from -r localGPT/requirements.txt (line 21))\n",
            "  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from -r localGPT/requirements.txt (line 23)) (8.1.7)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from -r localGPT/requirements.txt (line 24)) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r localGPT/requirements.txt (line 25)) (2.31.0)\n",
            "Collecting streamlit (from -r localGPT/requirements.txt (line 28))\n",
            "  Downloading streamlit-1.28.0-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Streamlit-extras (from -r localGPT/requirements.txt (line 29))\n",
            "  Downloading streamlit_extras-0.3.4-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from -r localGPT/requirements.txt (line 32)) (3.1.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.267->-r localGPT/requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.267->-r localGPT/requirements.txt (line 2)) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.267->-r localGPT/requirements.txt (line 2)) (3.8.6)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.267->-r localGPT/requirements.txt (line 2)) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.267->-r localGPT/requirements.txt (line 2))\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.21 (from langchain==0.0.267->-r localGPT/requirements.txt (line 2))\n",
            "  Downloading langsmith-0.0.53-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.267->-r localGPT/requirements.txt (line 2)) (2.8.7)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.267->-r localGPT/requirements.txt (line 2)) (1.23.5)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.267->-r localGPT/requirements.txt (line 2))\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.267->-r localGPT/requirements.txt (line 2)) (1.10.13)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.267->-r localGPT/requirements.txt (line 2)) (8.2.3)\n",
            "Collecting chroma-hnswlib==0.7.2 (from chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading chroma-hnswlib-0.7.2.tar.gz (31 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi<0.100.0,>=0.95.2 (from chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.6->-r localGPT/requirements.txt (line 3)) (4.5.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m121.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading onnxruntime-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypika>=0.48.9 (from chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.6->-r localGPT/requirements.txt (line 3)) (4.66.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.6->-r localGPT/requirements.txt (line 3)) (6.1.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->-r localGPT/requirements.txt (line 4)) (3.3.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->-r localGPT/requirements.txt (line 4)) (41.0.5)\n",
            "Collecting datasets (from auto-gptq==0.2.2->-r localGPT/requirements.txt (line 13))\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge (from auto-gptq==0.2.2->-r localGPT/requirements.txt (line 13))\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq==0.2.2->-r localGPT/requirements.txt (line 13)) (2.1.0+cu118)\n",
            "Collecting safetensors (from auto-gptq==0.2.2->-r localGPT/requirements.txt (line 13))\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r localGPT/requirements.txt (line 6)) (0.16.0+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r localGPT/requirements.txt (line 6)) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r localGPT/requirements.txt (line 6)) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r localGPT/requirements.txt (line 6)) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers->-r localGPT/requirements.txt (line 6))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r localGPT/requirements.txt (line 8)) (3.12.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r localGPT/requirements.txt (line 8)) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r localGPT/requirements.txt (line 8)) (23.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r localGPT/requirements.txt (line 9)) (2023.6.3)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured->-r localGPT/requirements.txt (line 15)) (5.2.0)\n",
            "Collecting filetype (from unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured->-r localGPT/requirements.txt (line 15)) (4.9.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured->-r localGPT/requirements.txt (line 15)) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured->-r localGPT/requirements.txt (line 15)) (4.11.2)\n",
            "Collecting emoji (from unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-iso639 (from unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading python_iso639-2023.6.15-py3-none-any.whl (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.1/275.1 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz (from unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading rapidfuzz-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff (from unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting onnx (from unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdf2image (from unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Collecting unstructured-inference==0.7.10 (from unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading unstructured_inference-0.7.10-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured.pytesseract>=0.3.12 (from unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading unstructured.pytesseract-0.3.12-py3-none-any.whl (14 kB)\n",
            "Collecting layoutparser[layoutmodels,tesseract] (from unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart (from unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15)) (4.8.0.76)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r localGPT/requirements.txt (line 20)) (5.9.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->-r localGPT/requirements.txt (line 24)) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask->-r localGPT/requirements.txt (line 24)) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->-r localGPT/requirements.txt (line 24)) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r localGPT/requirements.txt (line 25)) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r localGPT/requirements.txt (line 25)) (2023.7.22)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r localGPT/requirements.txt (line 28)) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit->-r localGPT/requirements.txt (line 28)) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r localGPT/requirements.txt (line 28)) (5.3.2)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r localGPT/requirements.txt (line 28)) (6.8.0)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r localGPT/requirements.txt (line 28)) (1.5.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r localGPT/requirements.txt (line 28)) (9.4.0)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r localGPT/requirements.txt (line 28)) (9.0.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r localGPT/requirements.txt (line 28)) (2.8.2)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r localGPT/requirements.txt (line 28)) (13.6.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r localGPT/requirements.txt (line 28)) (0.10.2)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r localGPT/requirements.txt (line 28)) (5.2)\n",
            "Collecting validators<1,>=0.2 (from streamlit->-r localGPT/requirements.txt (line 28))\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit->-r localGPT/requirements.txt (line 28))\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit->-r localGPT/requirements.txt (line 28))\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m132.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r localGPT/requirements.txt (line 28)) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit->-r localGPT/requirements.txt (line 28))\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints>=0.4 in /usr/local/lib/python3.10/dist-packages (from Streamlit-extras->-r localGPT/requirements.txt (line 29)) (0.4)\n",
            "Collecting htbuilder>=0.6.2 (from Streamlit-extras->-r localGPT/requirements.txt (line 29))\n",
            "  Downloading htbuilder-0.6.2-py3-none-any.whl (12 kB)\n",
            "Collecting markdownlit>=0.0.5 (from Streamlit-extras->-r localGPT/requirements.txt (line 29))\n",
            "  Downloading markdownlit-0.0.7-py3-none-any.whl (15 kB)\n",
            "INFO: pip is looking at multiple versions of streamlit-extras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting Streamlit-extras (from -r localGPT/requirements.txt (line 29))\n",
            "  Downloading streamlit_extras-0.3.2-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading streamlit_extras-0.3.1-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading streamlit_extras-0.3.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting htbuilder==0.6.1 (from Streamlit-extras->-r localGPT/requirements.txt (line 29))\n",
            "  Downloading htbuilder-0.6.1.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Streamlit-extras (from -r localGPT/requirements.txt (line 29))\n",
            "  Downloading streamlit_extras-0.2.7-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading streamlit_extras-0.2.6-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading streamlit_extras-0.2.5-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading streamlit_extras-0.2.4-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of streamlit-extras to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading streamlit_extras-0.2.3-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading streamlit_extras-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading streamlit_extras-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading streamlit_extras-0.2.0-py3-none-any.whl (36 kB)\n",
            "  Downloading streamlit_extras-0.1.5-py3-none-any.whl (34 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading streamlit_extras-0.1.4-py3-none-any.whl (34 kB)\n",
            "  Downloading streamlit_extras-0.1.3-py3-none-any.whl (33 kB)\n",
            "  Downloading streamlit_extras-0.1.2-py3-none-any.whl (28 kB)\n",
            "  Downloading streamlit_extras-0.1.1-py3-none-any.whl (27 kB)\n",
            "  Downloading streamlit_extras-0.0.9-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->-r localGPT/requirements.txt (line 32)) (1.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.267->-r localGPT/requirements.txt (line 2)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.267->-r localGPT/requirements.txt (line 2)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.267->-r localGPT/requirements.txt (line 2)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.267->-r localGPT/requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.267->-r localGPT/requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r localGPT/requirements.txt (line 28)) (4.19.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r localGPT/requirements.txt (line 28)) (0.12.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->-r localGPT/requirements.txt (line 4)) (1.16.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.267->-r localGPT/requirements.txt (line 2))\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.267->-r localGPT/requirements.txt (line 2))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r localGPT/requirements.txt (line 28))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit->-r localGPT/requirements.txt (line 28)) (3.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask->-r localGPT/requirements.txt (line 24)) (2.1.3)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.6->-r localGPT/requirements.txt (line 3)) (23.5.26)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.6->-r localGPT/requirements.txt (line 3)) (1.12)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit->-r localGPT/requirements.txt (line 28)) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb==0.4.6->-r localGPT/requirements.txt (line 3)) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->-r localGPT/requirements.txt (line 28)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->-r localGPT/requirements.txt (line 28)) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.267->-r localGPT/requirements.txt (line 2)) (3.0.0)\n",
            "Collecting huggingface_hub (from -r localGPT/requirements.txt (line 8))\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.2->-r localGPT/requirements.txt (line 13)) (3.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.2->-r localGPT/requirements.txt (line 13)) (2.1.0)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m121.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured->-r localGPT/requirements.txt (line 15)) (2.5)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets->auto-gptq==0.2.2->-r localGPT/requirements.txt (line 13))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq==0.2.2->-r localGPT/requirements.txt (line 13)) (3.4.1)\n",
            "Collecting multiprocess (from datasets->auto-gptq==0.2.2->-r localGPT/requirements.txt (line 13))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers->-r localGPT/requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r localGPT/requirements.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->-r localGPT/requirements.txt (line 4)) (2.21)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r localGPT/requirements.txt (line 28))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r localGPT/requirements.txt (line 28)) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r localGPT/requirements.txt (line 28)) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r localGPT/requirements.txt (line 28)) (0.10.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->-r localGPT/requirements.txt (line 28)) (0.1.2)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.6->-r localGPT/requirements.txt (line 3)) (3.7.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.267->-r localGPT/requirements.txt (line 2))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.6->-r localGPT/requirements.txt (line 3))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting iopath (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfplumber (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading pdfplumber-0.10.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytesseract (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Collecting effdet (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.6->-r localGPT/requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.6->-r localGPT/requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.6->-r localGPT/requirements.txt (line 3)) (1.1.3)\n",
            "Collecting timm>=0.9.2 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading timm-0.9.8-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15)) (2.0.7)\n",
            "Collecting omegaconf>=2.0 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading pypdfium2-4.22.0-py3-none-manylinux_2_17_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15)) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15)) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15)) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.10->unstructured->-r localGPT/requirements.txt (line 15)) (3.1.1)\n",
            "Building wheels for collected packages: auto-gptq, chroma-hnswlib, sentence-transformers, docx2txt, pypika, langdetect, iopath, antlr4-python3-runtime\n",
            "  Building wheel for auto-gptq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for auto-gptq: filename=auto_gptq-0.2.2-cp310-cp310-linux_x86_64.whl size=2861882 sha256=fbfe95d0993172df3548c8efa643480763554e18f356fd4e4aecca0e2fb719d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/f6/50/bb6ab784e7824cbf190a1a8205d91e6543287718fb21d5b033\n",
            "  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.2-cp310-cp310-linux_x86_64.whl size=2287386 sha256=92f3cc663d36c423b2ed476bb194ff099f0936ffb8eb7abaf56a6285f31c5064\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/2b/0d/ee457f6782f75315bb5828d5c2dc5639d471afbd44a830b9dc\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=443bd28295855dea51a49b2df796232c7d022e4fd6e2786eb0bb218084157eb9\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3959 sha256=9f1a6cc68c604712158f3e6b1b15c5b88b7231c27fec8e4514901b4853fb1a0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=3d7b9cfab811884da5e1048b1f372bd1700872be30b68ed816522f514f25b801\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=59c5cb84bf3b7f6037a3d7639d790943719916a48c1827e446d9368cf550a8de\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31530 sha256=37a0b106a3184f4f89216058a0325ce042a3b9f712243427f8aeb11e57f0a176\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=293381409ff12b8e2faeec17525fc56954ee3e92832bbe6af7cadcf499d0f7a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built auto-gptq chroma-hnswlib sentence-transformers docx2txt pypika langdetect iopath antlr4-python3-runtime\n",
            "Installing collected packages: sentencepiece, pypika, monotonic, InstructorEmbedding, filetype, faiss-cpu, docx2txt, bitsandbytes, antlr4-python3-runtime, websockets, watchdog, validators, uvloop, urllib3, unstructured.pytesseract, Streamlit-extras, smmap, safetensors, rouge, rapidfuzz, python-multipart, python-magic, python-iso639, python-dotenv, pytesseract, pypdfium2, pulsar-client, protobuf, portalocker, pdf2image, overrides, omegaconf, mypy-extensions, marshmallow, langdetect, humanfriendly, httptools, h11, emoji, dill, chroma-hnswlib, backoff, watchfiles, uvicorn, typing-inspect, starlette, pydeck, openapi-schema-pydantic, onnx, multiprocess, iopath, gitdb, coloredlogs, posthog, pdfminer.six, onnxruntime, langsmith, huggingface_hub, gitpython, fastapi, dataclasses-json, unstructured, tokenizers, timm, pdfplumber, langchain, datasets, accelerate, transformers, streamlit, layoutparser, effdet, chromadb, sentence-transformers, auto-gptq, unstructured-inference\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "tensorflow 2.14.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed InstructorEmbedding-1.0.1 Streamlit-extras-0.0.9 accelerate-0.24.0 antlr4-python3-runtime-4.9.3 auto-gptq-0.2.2 backoff-2.2.1 bitsandbytes-0.41.1 chroma-hnswlib-0.7.2 chromadb-0.4.6 coloredlogs-15.0.1 dataclasses-json-0.5.14 datasets-2.14.6 dill-0.3.7 docx2txt-0.8 effdet-0.4.1 emoji-2.8.0 faiss-cpu-1.7.4 fastapi-0.99.1 filetype-1.2.0 gitdb-4.0.11 gitpython-3.1.40 h11-0.14.0 httptools-0.6.1 huggingface_hub-0.17.3 humanfriendly-10.0 iopath-0.1.10 langchain-0.0.267 langdetect-1.0.9 langsmith-0.0.53 layoutparser-0.3.4 marshmallow-3.20.1 monotonic-1.6 multiprocess-0.70.15 mypy-extensions-1.0.0 omegaconf-2.3.0 onnx-1.15.0 onnxruntime-1.15.1 openapi-schema-pydantic-1.2.4 overrides-7.4.0 pdf2image-1.16.3 pdfminer.six-20221105 pdfplumber-0.10.3 portalocker-2.8.2 posthog-3.0.2 protobuf-3.20.2 pulsar-client-3.3.0 pydeck-0.8.1b0 pypdfium2-4.22.0 pypika-0.48.9 pytesseract-0.3.10 python-dotenv-1.0.0 python-iso639-2023.6.15 python-magic-0.4.27 python-multipart-0.0.6 rapidfuzz-3.4.0 rouge-1.0.1 safetensors-0.4.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 smmap-5.0.1 starlette-0.27.0 streamlit-1.28.0 timm-0.9.8 tokenizers-0.14.1 transformers-4.34.1 typing-inspect-0.9.0 unstructured-0.10.27 unstructured-inference-0.7.10 unstructured.pytesseract-0.3.12 urllib3-1.26.6 uvicorn-0.23.2 uvloop-0.19.0 validators-0.22.0 watchdog-3.0.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "\n",
        "\n",
        "# rm -r localGPT/DB/\n",
        "python localGPT/ingest.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QebiFrSjLTGx",
        "outputId": "b0f6eb97-4fb4-47f0-c082-776c0d7839b7"
      },
      "id": "QebiFrSjLTGx",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing: resiliency_test_rds.docx\n",
            "Importing: ops automation.docx\n",
            "Importing: Production bridge.docx\n",
            "Importing: Involve dev team on incident.docx\n",
            "Importing: nod validation.docx\n",
            "Importing: appd.docx\n",
            "Importing: low order.docx\n",
            "Importing: AWS-Platform Dashboard Jazz Deployment.docx\n",
            "Importing: documentation.docx\n",
            "Importing: daily_activity.docx\n",
            "/content/localGPT/SOURCE_DOCUMENTS/appd.docx loaded.\n",
            "\n",
            "/content/localGPT/SOURCE_DOCUMENTS/low order.docx loaded.\n",
            "\n",
            "/content/localGPT/SOURCE_DOCUMENTS/AWS-Platform Dashboard Jazz Deployment.docx loaded.\n",
            "\n",
            "/content/localGPT/SOURCE_DOCUMENTS/documentation.docx loaded.\n",
            "\n",
            "/content/localGPT/SOURCE_DOCUMENTS/daily_activity.docx loaded.\n",
            "\n",
            "/content/localGPT/SOURCE_DOCUMENTS/resiliency_test_rds.docx loaded.\n",
            "\n",
            "/content/localGPT/SOURCE_DOCUMENTS/ops automation.docx loaded.\n",
            "\n",
            "/content/localGPT/SOURCE_DOCUMENTS/Production bridge.docx loaded.\n",
            "\n",
            "/content/localGPT/SOURCE_DOCUMENTS/Involve dev team on incident.docx loaded.\n",
            "\n",
            "/content/localGPT/SOURCE_DOCUMENTS/nod validation.docx loaded.\n",
            "\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-10-29 12:43:58,003 - INFO - ingest.py:144 - Loading documents from /content/localGPT/SOURCE_DOCUMENTS\n",
            "2023-10-29 12:43:58,018 - INFO - ingest.py:44 - Loading document batch\n",
            "2023-10-29 12:43:58,018 - INFO - ingest.py:44 - Loading document batch\n",
            "2023-10-29 12:43:58,087 - INFO - ingest.py:153 - Loaded 10 documents from /content/localGPT/SOURCE_DOCUMENTS\n",
            "2023-10-29 12:43:58,087 - INFO - ingest.py:154 - Split into 45 chunks of text\n",
            "2023-10-29 12:44:00,096 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: hkunlp/instructor-large\n",
            "\rDownloading (…)c7233/.gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]\rDownloading (…)c7233/.gitattributes: 100%|██████████| 1.48k/1.48k [00:00<00:00, 7.48MB/s]\n",
            "\rDownloading (…)_Pooling/config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]\rDownloading (…)_Pooling/config.json: 100%|██████████| 270/270 [00:00<00:00, 2.20MB/s]\n",
            "\rDownloading (…)/2_Dense/config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]\rDownloading (…)/2_Dense/config.json: 100%|██████████| 116/116 [00:00<00:00, 867kB/s]\n",
            "\rDownloading pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]\rDownloading pytorch_model.bin: 100%|██████████| 3.15M/3.15M [00:00<00:00, 37.5MB/s]\n",
            "\rDownloading (…)9fb15c7233/README.md:   0%|          | 0.00/66.3k [00:00<?, ?B/s]\rDownloading (…)9fb15c7233/README.md: 100%|██████████| 66.3k/66.3k [00:00<00:00, 18.3MB/s]\n",
            "\rDownloading (…)b15c7233/config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]\rDownloading (…)b15c7233/config.json: 100%|██████████| 1.53k/1.53k [00:00<00:00, 9.64MB/s]\n",
            "\rDownloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]\rDownloading (…)ce_transformers.json: 100%|██████████| 122/122 [00:00<00:00, 837kB/s]\n",
            "\rDownloading pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]\rDownloading pytorch_model.bin:   2%|▏         | 31.5M/1.34G [00:00<00:04, 265MB/s]\rDownloading pytorch_model.bin:   5%|▍         | 62.9M/1.34G [00:00<00:04, 260MB/s]\rDownloading pytorch_model.bin:   7%|▋         | 94.4M/1.34G [00:00<00:04, 264MB/s]\rDownloading pytorch_model.bin:   9%|▉         | 126M/1.34G [00:00<00:04, 267MB/s] \rDownloading pytorch_model.bin:  12%|█▏        | 157M/1.34G [00:00<00:04, 246MB/s]\rDownloading pytorch_model.bin:  14%|█▍        | 189M/1.34G [00:00<00:04, 238MB/s]\rDownloading pytorch_model.bin:  16%|█▋        | 220M/1.34G [00:00<00:04, 226MB/s]\rDownloading pytorch_model.bin:  19%|█▉        | 252M/1.34G [00:01<00:04, 223MB/s]\rDownloading pytorch_model.bin:  21%|██        | 283M/1.34G [00:01<00:04, 218MB/s]\rDownloading pytorch_model.bin:  23%|██▎       | 315M/1.34G [00:01<00:04, 221MB/s]\rDownloading pytorch_model.bin:  26%|██▌       | 346M/1.34G [00:01<00:04, 216MB/s]\rDownloading pytorch_model.bin:  28%|██▊       | 377M/1.34G [00:01<00:04, 218MB/s]\rDownloading pytorch_model.bin:  31%|███       | 409M/1.34G [00:01<00:04, 223MB/s]\rDownloading pytorch_model.bin:  33%|███▎      | 440M/1.34G [00:01<00:03, 227MB/s]\rDownloading pytorch_model.bin:  35%|███▌      | 472M/1.34G [00:02<00:04, 205MB/s]\rDownloading pytorch_model.bin:  37%|███▋      | 493M/1.34G [00:02<00:04, 202MB/s]\rDownloading pytorch_model.bin:  38%|███▊      | 514M/1.34G [00:02<00:04, 197MB/s]\rDownloading pytorch_model.bin:  40%|███▉      | 535M/1.34G [00:02<00:04, 199MB/s]\rDownloading pytorch_model.bin:  42%|████▏     | 566M/1.34G [00:02<00:03, 204MB/s]\rDownloading pytorch_model.bin:  44%|████▍     | 587M/1.34G [00:02<00:03, 198MB/s]\rDownloading pytorch_model.bin:  45%|████▌     | 608M/1.34G [00:03<00:07, 91.6MB/s]\rDownloading pytorch_model.bin:  47%|████▋     | 629M/1.34G [00:03<00:06, 106MB/s] \rDownloading pytorch_model.bin:  49%|████▊     | 650M/1.34G [00:03<00:05, 122MB/s]\rDownloading pytorch_model.bin:  51%|█████     | 682M/1.34G [00:03<00:04, 149MB/s]\rDownloading pytorch_model.bin:  53%|█████▎    | 713M/1.34G [00:03<00:03, 169MB/s]\rDownloading pytorch_model.bin:  56%|█████▌    | 744M/1.34G [00:03<00:03, 186MB/s]\rDownloading pytorch_model.bin:  58%|█████▊    | 776M/1.34G [00:04<00:03, 147MB/s]\rDownloading pytorch_model.bin:  60%|██████    | 807M/1.34G [00:04<00:03, 162MB/s]\rDownloading pytorch_model.bin:  62%|██████▏   | 828M/1.34G [00:04<00:03, 165MB/s]\rDownloading pytorch_model.bin:  63%|██████▎   | 849M/1.34G [00:04<00:02, 170MB/s]\rDownloading pytorch_model.bin:  65%|██████▍   | 870M/1.34G [00:04<00:02, 176MB/s]\rDownloading pytorch_model.bin:  67%|██████▋   | 891M/1.34G [00:04<00:02, 181MB/s]\rDownloading pytorch_model.bin:  68%|██████▊   | 912M/1.34G [00:04<00:02, 188MB/s]\rDownloading pytorch_model.bin:  70%|███████   | 944M/1.34G [00:05<00:01, 203MB/s]\rDownloading pytorch_model.bin:  73%|███████▎  | 975M/1.34G [00:05<00:01, 203MB/s]\rDownloading pytorch_model.bin:  74%|███████▍  | 996M/1.34G [00:05<00:01, 196MB/s]\rDownloading pytorch_model.bin:  76%|███████▌  | 1.02G/1.34G [00:05<00:01, 194MB/s]\rDownloading pytorch_model.bin:  77%|███████▋  | 1.04G/1.34G [00:05<00:01, 190MB/s]\rDownloading pytorch_model.bin:  79%|███████▉  | 1.06G/1.34G [00:05<00:01, 194MB/s]\rDownloading pytorch_model.bin:  81%|████████  | 1.08G/1.34G [00:05<00:01, 196MB/s]\rDownloading pytorch_model.bin:  83%|████████▎ | 1.11G/1.34G [00:05<00:01, 205MB/s]\rDownloading pytorch_model.bin:  85%|████████▌ | 1.14G/1.34G [00:06<00:00, 209MB/s]\rDownloading pytorch_model.bin:  88%|████████▊ | 1.17G/1.34G [00:06<00:00, 212MB/s]\rDownloading pytorch_model.bin:  90%|█████████ | 1.21G/1.34G [00:06<00:00, 216MB/s]\rDownloading pytorch_model.bin:  92%|█████████▏| 1.24G/1.34G [00:06<00:00, 224MB/s]\rDownloading pytorch_model.bin:  95%|█████████▍| 1.27G/1.34G [00:06<00:00, 226MB/s]\rDownloading pytorch_model.bin:  97%|█████████▋| 1.30G/1.34G [00:06<00:00, 215MB/s]\rDownloading pytorch_model.bin:  99%|█████████▉| 1.33G/1.34G [00:06<00:00, 208MB/s]\rDownloading pytorch_model.bin: 100%|██████████| 1.34G/1.34G [00:06<00:00, 193MB/s]\n",
            "\rDownloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]\rDownloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 208kB/s]\n",
            "\rDownloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]\rDownloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 11.5MB/s]\n",
            "\rDownloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]\rDownloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 164MB/s]\n",
            "\rDownloading (…)c7233/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]\rDownloading (…)c7233/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 40.3MB/s]\n",
            "\rDownloading (…)okenizer_config.json:   0%|          | 0.00/2.41k [00:00<?, ?B/s]\rDownloading (…)okenizer_config.json: 100%|██████████| 2.41k/2.41k [00:00<00:00, 12.9MB/s]\n",
            "\rDownloading (…)15c7233/modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]\rDownloading (…)15c7233/modules.json: 100%|██████████| 461/461 [00:00<00:00, 3.42MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e4c5aae2-41bb-4089-b4af-d2feb3bcb48a",
      "metadata": {
        "id": "e4c5aae2-41bb-4089-b4af-d2feb3bcb48a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import click\n",
        "import torch\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  # for streaming response\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "from localGPT.prompt_template_utils import get_prompt_template\n",
        "\n",
        "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.vectorstores import Chroma\n",
        "from transformers import (\n",
        "    GenerationConfig,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "# from localGPT.load_models import (\n",
        "#     load_quantized_model_gguf_ggml,\n",
        "#     load_quantized_model_qptq,\n",
        "#     load_full_model,\n",
        "# )\n",
        "\n",
        "# from localGPT.constants import (\n",
        "#     EMBEDDING_MODEL_NAME,\n",
        "#     PERSIST_DIRECTORY,\n",
        "#     MODEL_ID,\n",
        "#     MODEL_BASENAME,\n",
        "#     MAX_NEW_TOKENS,\n",
        "#     MODELS_PATH,\n",
        "# )\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0080ef43-f42c-4f8a-8582-0d5c11c8b5a2",
      "metadata": {
        "id": "0080ef43-f42c-4f8a-8582-0d5c11c8b5a2"
      },
      "outputs": [],
      "source": [
        "# Finetuned quantized model\n",
        "\n",
        "# MODEL_ID = \"ggml-model-q4_k_m_new.gguf\"\n",
        "# MODEL_BASENAME = \"ggml-model-q4_k_m_new.gguf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "05f343a3-1c01-4362-a1c0-5bf93cf32735",
      "metadata": {
        "id": "05f343a3-1c01-4362-a1c0-5bf93cf32735"
      },
      "outputs": [],
      "source": [
        "# Pretrained quantized model\n",
        "\n",
        "MODEL_ID = \"TheBloke/Llama-2-7b-Chat-GGUF\"\n",
        "MODEL_BASENAME = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
        "EMBEDDING_MODEL_NAME = \"hkunlp/instructor-large\"\n",
        "\n",
        "import os\n",
        "\n",
        "# from dotenv import load_dotenv\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/excel.html?highlight=xlsx#microsoft-excel\n",
        "from langchain.document_loaders import CSVLoader, PDFMinerLoader, TextLoader, UnstructuredExcelLoader, Docx2txtLoader\n",
        "from langchain.document_loaders import UnstructuredFileLoader, UnstructuredMarkdownLoader\n",
        "\n",
        "\n",
        "# load_dotenv()\n",
        "ROOT_DIRECTORY = \"localGPT\"\n",
        "\n",
        "# Define the folder for storing database\n",
        "SOURCE_DIRECTORY = f\"{ROOT_DIRECTORY}/SOURCE_DOCUMENTS\"\n",
        "\n",
        "PERSIST_DIRECTORY = f\"{ROOT_DIRECTORY}/DB\"\n",
        "\n",
        "MODELS_PATH = \"./models\"\n",
        "\n",
        "# Can be changed to a specific number\n",
        "INGEST_THREADS = os.cpu_count() or 8\n",
        "\n",
        "# Define the Chroma settings\n",
        "CHROMA_SETTINGS = Settings(\n",
        "    anonymized_telemetry=False,\n",
        "    is_persistent=True,\n",
        ")\n",
        "\n",
        "# Context Window and Max New Tokens\n",
        "CONTEXT_WINDOW_SIZE = 4096\n",
        "MAX_NEW_TOKENS = CONTEXT_WINDOW_SIZE  # int(CONTEXT_WINDOW_SIZE/4)\n",
        "\n",
        "#### If you get a \"not enough space in the buffer\" error, you should reduce the values below, start with half of the original values and keep halving the value until the error stops appearing\n",
        "\n",
        "N_GPU_LAYERS = 100  # Llama-2-70B has 83 layers\n",
        "N_BATCH = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "25c5ecbe-b93c-4985-a5fe-8b18003bcf33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25c5ecbe-b93c-4985-a5fe-8b18003bcf33",
        "outputId": "abe049c0-4caf-47d0-f842-74b910eba56c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ],
      "source": [
        "embeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": \"cpu\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e7ddedff-2d4c-4048-b72c-e08edb9da58e",
      "metadata": {
        "id": "e7ddedff-2d4c-4048-b72c-e08edb9da58e"
      },
      "outputs": [],
      "source": [
        "db = Chroma(\n",
        "        persist_directory=PERSIST_DIRECTORY,\n",
        "        embedding_function=embeddings,\n",
        "    )\n",
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c3df0e7a-5f39-4914-b900-39d3436ec76b",
      "metadata": {
        "id": "c3df0e7a-5f39-4914-b900-39d3436ec76b"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "use_history = False\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "system_prompt = \"\"\"You are a helpful assistant, you will use the provided context to answer user questions.\n",
        "Read the given context before answering questions and think step by step. If you can not answer a user question based on\n",
        "the provided context, inform the user. Do not use any other information for answering user. Provide a detailed answer to the question.\"\"\"\n",
        "\n",
        "\n",
        "# system_prompt = \"\"\"You are a helpful assistant, you will use the provided context to answer user questions.\n",
        "# Read the given context before answering questions.\n",
        "# If it contains a sequence of instructions, re-write those instructions in the following format:\n",
        "\n",
        "# Step 1 - ...\n",
        "# Step 2 - …\n",
        "# …\n",
        "# Step N - …\n",
        "\n",
        "# If the text does not contain a sequence of instructions, then only write the summary of the text.\n",
        "# If you can not answer a user question based on the provided context, inform the user.\n",
        "# Do not use any other information for answering user. Provide a detailed answer to the question.\"\"\"\n",
        "\n",
        "\n",
        "# prompt, memory = get_prompt_template(promptTemplate_type=\"llama\", history=use_history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5b88b11b-1ba3-409d-ace7-65e887d9e8d6",
      "metadata": {
        "id": "5b88b11b-1ba3-409d-ace7-65e887d9e8d6"
      },
      "outputs": [],
      "source": [
        "instruction = \"\"\"\n",
        "            Context: {context}\n",
        "            User: {question}\"\"\"\n",
        "SYSTEM_PROMPT = B_SYS + system_prompt + E_SYS\n",
        "prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
        "memory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain.llms import LlamaCpp\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    LlamaForCausalLM,\n",
        "    LlamaTokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "OmcHK6yIVd-T"
      },
      "id": "OmcHK6yIVd-T",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = hf_hub_download(\n",
        "            repo_id=MODEL_ID,\n",
        "            filename=MODEL_BASENAME,\n",
        "            resume_download=True,\n",
        "            cache_dir=MODELS_PATH,\n",
        "        )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "41384b5513b340f8802f50c025b92825",
            "e708affcac6f4b3eb372e70dd7c99879",
            "5abc3467a24449058e52f0ff8c8472de",
            "352134394aa446129d9ba29e06c160af",
            "3ab4d94f9792415485396b7137296608",
            "841174c277e9438fa827eee1afa575cc",
            "650e3d4084b146aca2605cac172ce4cb",
            "5e088071194d438d8ee25c18e25f1cb1",
            "c9efe5e4197b4c778bdfc5c18430e694",
            "21f5912b1c6947ffbf68d6ddfa894ba0",
            "34d6762056544310a034f1cf7f9cfa77"
          ]
        },
        "id": "x8kpVce9Vmla",
        "outputId": "fc2c596f-2c10-4188-9e79-fa89191ea77f"
      },
      "id": "x8kpVce9Vmla",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)-7b-chat.Q4_K_M.gguf:   0%|          | 0.00/4.08G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41384b5513b340f8802f50c025b92825"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OI2cORoXF2s",
        "outputId": "c3670105-351e-4c26-9ee0-58b7de5999fc"
      },
      "id": "3OI2cORoXF2s",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.11.tar.gz (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.11-cp310-cp310-manylinux_2_35_x86_64.whl size=1023486 sha256=b4568484b16501e68438bd33063aa054f9fd2be22462b1389b846963ce1209a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/42/77/a3ab0d02700427ea364de5797786c0272779dce795f62c3bc2\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dsVI9DeGwwtu"
      },
      "id": "dsVI9DeGwwtu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kwargs = {\n",
        "            \"model_path\": model_path,\n",
        "            \"n_ctx\": CONTEXT_WINDOW_SIZE,\n",
        "            \"max_tokens\": MAX_NEW_TOKENS,\n",
        "            \"n_batch\": N_BATCH,  # set this based on your GPU & CPU RAM\n",
        "        }\n",
        "llm = LlamaCpp(**kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrIuupzEV9mz",
        "outputId": "9fe70a17-c54f-41e2-db87-d74e3a7d2dc8"
      },
      "id": "rrIuupzEV9mz",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "aaeb3833-a6bd-4b93-86d8-10eeccc1a457",
      "metadata": {
        "id": "aaeb3833-a6bd-4b93-86d8-10eeccc1a457"
      },
      "outputs": [],
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\",  # try other chains types as well. refine, map_reduce, map_rerank\n",
        "            retriever=retriever,\n",
        "            return_source_documents=True,  # verbose=True,\n",
        "            callbacks=callback_manager,\n",
        "            chain_type_kwargs={\n",
        "                \"prompt\": prompt,\n",
        "            },\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5f55e360-3d34-4139-818a-75fab34c7f24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f55e360-3d34-4139-818a-75fab34c7f24",
        "outputId": "bbdb0d51-32e9-4e91-f63c-9b9def4c82c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Great, thank you for your question! In the context of the Standard Operating Procedure you provided, when there is an order trend low scenario, the following steps should be taken:\n",
            "1. Confirm Low Order Volume: Verify that there is indeed a decrease in order volume compared to previous periods. (Step 1 of the procedure)\n",
            "2. Review Historical Data: Analyze historical order data to identify any patterns, seasonality, or anomalies that might explain the decrease. (Step 2 of the procedure)\n",
            "3. Examine Production Services: Review the status of critical microservices and components responsible for order processing, payment, and inventory management. (Step 3 of the procedure)\n",
            "4. Performance Metrics: Analyze CPU usage, memory consumption, network activity, and response times for each service. (Step 4 of the procedure)\n",
            "5. Issue Mitigation: Implement corrective actions based on validated hypotheses. (Step 6 of the procedure)\n",
            "6. Document Findings: Maintain detailed records of findings, actions taken, and outcomes for future reference. (Step 7 of the procedure)\n",
            "By following these steps, the Amazon Operations Team can identify and address any production service issues that might be affecting order processing, thus resolving the low order volume scenario.\n"
          ]
        }
      ],
      "source": [
        "# quantized model finetuned\n",
        "query = \"What steps to do when there is a order trend low scenerio ?\"\n",
        "res = qa(query)\n",
        "answer, docs = res[\"result\"], res[\"source_documents\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "40f79c13-3818-40dd-a381-7889ea8f6dc3",
      "metadata": {
        "id": "40f79c13-3818-40dd-a381-7889ea8f6dc3"
      },
      "outputs": [],
      "source": [
        "from typing import Sequence, Optional\n",
        "from langchain.prompts import (\n",
        "    PromptTemplate,\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.llms import OpenAI\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "\n",
        "class Task(BaseModel):\n",
        "    task_name: str\n",
        "\n",
        "class Tasks(BaseModel):\n",
        "    \"\"\"Identifying information about all tasks in a text.\"\"\"\n",
        "    people: Sequence[Task]\n",
        "\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Tasks)\n",
        "\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "\n",
        "\n",
        "instruction = \"\"\"\\\n",
        "For the following context, extract the following information:\n",
        "\n",
        "task_name: Extract any automation task name that can be used for the user query and output them as json in following format.\n",
        "json format ```\n",
        "{{{{\n",
        "    \"task_name\": string \\ name of the automation task\n",
        "}}}}\n",
        "context: {context}\n",
        "User: {question}\n",
        "\"\"\"\n",
        "# SYSTEM_PROMPT = B_SYS + system_prompt + E_SYS\n",
        "# prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "prompt_template = B_INST + instruction + E_INST\n",
        "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
        "memory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "a2463a97-be51-46a7-99db-f335363ad0b8",
      "metadata": {
        "id": "a2463a97-be51-46a7-99db-f335363ad0b8"
      },
      "outputs": [],
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\",  # try other chains types as well. refine, map_reduce, map_rerank\n",
        "            retriever=retriever,\n",
        "            return_source_documents=True,  # verbose=True,\n",
        "            callbacks=callback_manager,\n",
        "            chain_type_kwargs={\n",
        "                \"prompt\": prompt,\n",
        "            },\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "6a95b751-ec3e-4590-884c-87c5cf006da6",
      "metadata": {
        "id": "6a95b751-ec3e-4590-884c-87c5cf006da6",
        "outputId": "0a13acce-917c-4b7c-be3a-2cfe6b49fad2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Based on the provided context, the following information can be extracted:\n",
            "Task Name:\n",
            "* Low Order Sanity AO\n",
            "* Validate External API Response Time\n",
            "* Load Balancing and Scaling\n",
            "* Security and DDoS\n",
            "\n",
            "Context:\n",
            "* OPS Automation Tasks\n",
            "* Objective: Different automation tasks that the ops team can use.\n",
            "\n",
            "Please find the extracted information in the format of a JSON object below:\n",
            "{{\n",
            "    \"task_name\": [\n",
            "        \"Low Order Sanity AO\",\n",
            "        \"Validate External API Response Time\",\n",
            "        \"Load Balancing and Scaling\",\n",
            "        \"Security and DDoS\"\n",
            "\n",
            "    ]\n",
            "}}\n"
          ]
        }
      ],
      "source": [
        "query = \"Which automation task should we run for low order trend scenerio\"\n",
        "res = qa(query)\n",
        "answer, docs = res[\"result\"], res[\"source_documents\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs = retriever.invoke(\"Which automation task should we run for low order trend scenerio\")\n",
        "print(retrieved_docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcxFzMYCuwVY",
        "outputId": "bc3e4c2f-1281-4628-99e8-6cbc10e9000c"
      },
      "id": "ZcxFzMYCuwVY",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OPS Automation Tasks :\n",
            "\n",
            "Objective: Different automation tasks ops team can use.\n",
            "\n",
            "Low Order Sanity AO:\n",
            "\n",
            "\t\tName: Low Order Sanity AO\n",
            "\n",
            "\t\tDescription: This task involves running an automation pipeline to check if both EIP (Enterprise Integration Patterns) and FRP (Functional Requirement Parameters) flows are passed for a low order trend scenario.\n",
            "\n",
            "\t\tRepository: The automation pipeline for this task is configured in the 'APP1-ops-synthetic-monitoring' repository.\n",
            "\n",
            "\t\tSchedule: The schedule for this automation pipeline is configured within the 'APP1-ops-synthetic-monitoring' repository.\n",
            "\n",
            "\t\tValidate External API Response Time:\n",
            "\n",
            "\t\tName: Validate External API Response Time\n",
            "\n",
            "\t\tDescription: This task involves running an automation pipeline to check if the response time or failure rate is above a threshold value for a low order trend scenario, likely involving external API calls.\n",
            "\n",
            "\t\tRepository: The automation pipeline for this task is configured in the 'APP1-ops-validator-pipelines' repository.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import create_extraction_chain\n",
        "\n",
        "# Schema\n",
        "schema = {\n",
        "    \"properties\": {\n",
        "        \"name\": {\"type\": \"string\"},\n",
        "    },\n",
        "    \"required\": [\"name\"],\n",
        "}\n",
        "\n",
        "# Input\n",
        "inp = retrieved_docs[0].page_content\n",
        "\n",
        "# Run chain\n",
        "chain = create_extraction_chain(schema, llm)\n",
        "chain.run(inp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "B9W7EXvJvGl8",
        "outputId": "147a49e6-cdcb-4921-c73e-c1741cfc6475"
      },
      "id": "B9W7EXvJvGl8",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-29ce06e1fdf0>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Run chain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_extraction_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             outputs = (\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;34m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    466\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             ]\n\u001b[0;32m--> 598\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    599\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             output = (\n\u001b[0;32m--> 491\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    492\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             text = (\n\u001b[0;32m--> 977\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/llamacpp.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;31m# and return the combined strings from the first choices's text:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mcombined_text_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             for chunk in self._stream(\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             ):\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/llamacpp.py\u001b[0m in \u001b[0;36m_stream\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \"\"\"\n\u001b[1;32m    302\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mlogprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logprobs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Llama.__call__() got an unexpected keyword argument 'functions'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser.parse(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "_2lWWGVplBLB",
        "outputId": "0801dff8-47ee-437f-f849-db29dd6f738a"
      },
      "id": "_2lWWGVplBLB",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutputParserException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mjson_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mjson_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mkw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parse_constant'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-79d4cb9894b7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Failed to parse {name} from completion {text}. Got: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOutputParserException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_format_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutputParserException\u001b[0m: Failed to parse Tasks from completion   Based on the provided context, the following information can be extracted:\nTask Name:\n* Low Order Sanity AO\n* Validate External API Response Time\n* Load Balancing and Scaling\n* Security and DDoS\n\nContext:\n* OPS Automation Tasks\n* Objective: Different automation tasks that the ops team can use.\n\nPlease find the extracted information in the format of a JSON object below:\n{{\n    \"task_name\": [\n        \"Low Order Sanity AO\",\n        \"Validate External API Response Time\",\n        \"Load Balancing and Scaling\",\n        \"Security and DDoS\"\n\n    ]\n}}. Got: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1YzwreGwzMu",
        "outputId": "a7d45aaf-b1ae-4648-94f0-33b6a037c06b"
      },
      "id": "I1YzwreGwzMu",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.8.54-py3-none-any.whl (795 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/795.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/795.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m795.5/795.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2.0.22)\n",
            "Collecting aiostream<0.6.0,>=0.5.2 (from llama-index)\n",
            "  Downloading aiostream-0.5.2-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.5.14)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2023.6.0)\n",
            "Collecting langchain>=0.0.303 (from llama-index)\n",
            "  Downloading langchain-0.0.325-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.8)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.23.5)\n",
            "Collecting openai>=0.26.4 (from llama-index)\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index)\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.5.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.9.0)\n",
            "Requirement already satisfied: urllib3<2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.26.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->llama-index) (3.20.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index) (1.14.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (6.0.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (4.0.3)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain>=0.0.303->llama-index)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.52 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (0.0.53)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (2.31.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (4.66.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (1.1.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain>=0.0.303->llama-index)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->llama-index) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.303->llama-index) (2023.7.22)\n",
            "Installing collected packages: jsonpointer, deprecated, aiostream, tiktoken, jsonpatch, openai, langchain, llama-index\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.0.267\n",
            "    Uninstalling langchain-0.0.267:\n",
            "      Successfully uninstalled langchain-0.0.267\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiostream-0.5.2 deprecated-1.2.14 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.325 llama-index-0.8.54 openai-0.28.1 tiktoken-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.program import (\n",
        "    BasePydanticProgram,\n",
        "    DFFullProgram,\n",
        "    DataFrame,\n",
        "    DataFrameRowsOnly,\n",
        ")\n",
        "\n",
        "program = BasePydanticProgram(\n",
        "    output_cls=DataFrame,\n",
        "    llm=llm,\n",
        "    prompt_template_str=(\n",
        "        \"Please extract the following query into a structured data according\"\n",
        "        \" to: {input_str}.Please extract both the set of column names and a\"\n",
        "        \" set of rows.\"\n",
        "    ),\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "response_obj = program(\n",
        "    input_str=\"\"\"My name is John and I am 25 years old. I live in\n",
        "        New York and I like to play basketball. His name is\n",
        "        Mike and he is 30 years old. He lives in San Francisco\n",
        "        and he likes to play baseball. Sarah is 20 years old\n",
        "        and she lives in Los Angeles. She likes to play tennis.\n",
        "        Her name is Mary and she is 35 years old.\n",
        "        She lives in Chicago.\"\"\"\n",
        ")\n",
        "response_obj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "Z0LlTonGxIgQ",
        "outputId": "1c2f4f80-01b4-4f34-e7f3-796deaf2f8d6"
      },
      "id": "Z0LlTonGxIgQ",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-0443f8e72ee6>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m program = BasePydanticProgram(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0moutput_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BasePydanticProgram() takes no arguments"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f97a6e2-5fa8-48e7-8630-1dbdfa4f929d",
      "metadata": {
        "id": "5f97a6e2-5fa8-48e7-8630-1dbdfa4f929d",
        "outputId": "4f2f129f-5ebf-4371-c81a-4fce83cd8775"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Based on the provided context and user query, the following automation tasks can be identified as relevant for the low order trend scenario:\n",
            "JSON Output:\n",
            "{\n",
            "\"task_name\": \"Low Order Sanity AO\",\n",
            "\"value\": \"APP1-ops-synthetic-monitoring\"\n",
            "}\n",
            "{\n",
            "\"task_name\": \"Validate External API Response time\",\n",
            "\"value\": \"APP1-ops-validator-pipelines\"\n",
            "}\n",
            "These tasks are identified based on the provided user query, which mentions the scenario of low order trend. The automation tasks listed above are relevant for this scenario, as they are designed to monitor and validate the performance of external APIs and flows in the APP1 environment. By running these tasks, the OPS team can identify any issues or anomalies that may be impacting the order trend and take corrective action accordingly.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time = 58653.81 ms\n",
            "llama_print_timings:      sample time =   223.70 ms /   194 runs   (    1.15 ms per token,   867.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =  7368.94 ms /    60 tokens (  122.82 ms per token,     8.14 tokens per second)\n",
            "llama_print_timings:        eval time = 31826.27 ms /   193 runs   (  164.90 ms per token,     6.06 tokens per second)\n",
            "llama_print_timings:       total time = 40084.63 ms\n"
          ]
        }
      ],
      "source": [
        "query = \"Find any automation that can be used for query : 'What are the steps for low order trend scenerio'\"\n",
        "res = qa(query)\n",
        "answer, docs = res[\"result\"], res[\"source_documents\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7abc137f-48f4-49c1-adfe-020c529f9937",
      "metadata": {
        "id": "7abc137f-48f4-49c1-adfe-020c529f9937",
        "outputId": "39fc8672-da56-44ba-8c95-7c11733ca401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Based on the provided context, the following are the task names that can be used for the user query:\n",
            "* \"Low order Sanity AO\"\n",
            "* \"Validate External API Response time\"\n",
            "\n",
            "The output in JSON format is:\n",
            "{\n",
            "\"task_name\": [\"Low order Sanity AO\", \"Validate External API Response time\"]\n",
            "}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time = 58601.99 ms\n",
            "llama_print_timings:      sample time =    90.51 ms /    80 runs   (    1.13 ms per token,   883.85 tokens per second)\n",
            "llama_print_timings: prompt eval time = 117849.35 ms /  1000 tokens (  117.85 ms per token,     8.49 tokens per second)\n",
            "llama_print_timings:        eval time = 13006.12 ms /    79 runs   (  164.63 ms per token,     6.07 tokens per second)\n",
            "llama_print_timings:       total time = 131204.30 ms\n"
          ]
        }
      ],
      "source": [
        "query = \"Find any automation that can be used for query : 'What are the steps for low order trend scenerio'\"\n",
        "res = qa(query)\n",
        "answer, docs = res[\"result\"], res[\"source_documents\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b07e207f-6f22-4f13-b5bd-db0a5bf10407",
      "metadata": {
        "id": "b07e207f-6f22-4f13-b5bd-db0a5bf10407",
        "outputId": "cf286c61-fb70-4c75-d11a-722dffb12b40"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'LlamaCpp' object has no attribute '_model'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m B_INST \u001b[38;5;241m+\u001b[39m instruction \u001b[38;5;241m+\u001b[39m E_INST\n\u001b[1;32m     36\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m], template\u001b[38;5;241m=\u001b[39mprompt_template)\n\u001b[0;32m---> 38\u001b[0m logits_processors \u001b[38;5;241m=\u001b[39m LogitsProcessorList([build_llamacpp_logits_processor(\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m, JsonSchemaParser(AnswerFormat\u001b[38;5;241m.\u001b[39mschema()))])\n\u001b[1;32m     40\u001b[0m qa \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[1;32m     41\u001b[0m             llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     42\u001b[0m             chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# try other chains types as well. refine, map_reduce, map_rerank\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m             },\n\u001b[1;32m     50\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LlamaCpp' object has no attribute '_model'"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "from typing import Optional\n",
        "from llama_cpp import LogitsProcessorList\n",
        "from lmformatenforcer import CharacterLevelParser, JsonSchemaParser\n",
        "from lmformatenforcer.integrations.llamacpp import build_llamacpp_logits_processor\n",
        "\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\\n",
        "\"\"\"\n",
        "\n",
        "instruction = \"\"\"\\\n",
        "For the following context and user query, extract the following information:\n",
        "\n",
        "context: {context}\n",
        "User: {question}\n",
        "\n",
        "task_name: Extract any automation task name that can be used for the user query and output them as a comma separated Python list.\n",
        "\n",
        "You MUST answer using the following json schema:\n",
        "task_name : str\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_prompt(message: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
        "    return f'<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{message} [/INST]'\n",
        "\n",
        "class AnswerFormat(BaseModel):\n",
        "    task_name: str\n",
        "\n",
        "# question = 'Please give me information about Michael Jordan. You MUST answer using the following json schema: '\n",
        "# question_with_schema = f'{question}{AnswerFormat.schema_json()}'\n",
        "schema = AnswerFormat.schema_json()\n",
        "prompt = get_prompt(question_with_schema)\n",
        "prompt_template = B_INST + instruction + E_INST\n",
        "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
        "\n",
        "logits_processors = LogitsProcessorList([build_llamacpp_logits_processor(llm._model, JsonSchemaParser(AnswerFormat.schema()))])\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\",  # try other chains types as well. refine, map_reduce, map_rerank\n",
        "            retriever=retriever,\n",
        "            return_source_documents=True,  # verbose=True,\n",
        "            callbacks=callback_manager,\n",
        "            chain_type_kwargs={\n",
        "                \"prompt\": prompt,\n",
        "                \"logits_processor\": logits_processors,\n",
        "            },\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "538dc04b-2c45-4903-a5f8-22eddf384922",
      "metadata": {
        "id": "538dc04b-2c45-4903-a5f8-22eddf384922",
        "outputId": "d389b7a0-eae1-4fed-933b-398ca149a9bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Based on the provided context and user query, here is the information extracted:\n",
            "Context: OPS Automation Tasks\n",
            "APP1-ops-synthetic-monitoring Pipeline:\n",
            "* When low/no order alert is received, ops team can run task \"Low order Sanity AO\" automation pipeline schedule configured in \"APP1-ops-synthetic-monitoring\" repo. Result will be published to slack. Check if both eip and frp flow is passed.\n",
            "APP1-ops-validator-pipelines:\n",
            "* When low/no order alert is received, ops team can run task \"Validate External API Response time\" automation pipeline schedule configured in \"APP1-ops-validator-pipelines\" repo. If response time or failure rate is above threshold value it will be marked validation failed in the result that is published to slack.\n",
            "OPS Automation Tasks:\n",
            "* When low/no TMO order alert is received, ops team can run \"TMO Sanity AO\" pipeline schedule configured in \"APP1-ops-synthetic-monitoring\" repo. Result will be published to slack. Check if both eip and frp flow is passed.\n",
            "* When low/no TMO order alert is received, ops team can run \"Validate External API Response time\" pipeline schedule configured in \"APP1-ops-validator-pipelines\" repo. If response time or failure rate is above threshold value it will be marked validation failed in the result that is published to slack.\n",
            "In case of order trend dip or pyro alert on low order trend low, as an initial triage:\n",
            "Step 1: Try to understand if it impacts any specific flow. This can be checked with APP1 - hourly order trend dashboard. If you see dip in all or any specific orders, you can identify them based on this dashboard. Can also use order summary dashboard.\n",
            "Step 2: Quick checks\n",
            "* Any alerts indicating increased error rates or response times?\n",
            "* Any pod restart alerts?\n",
            "* Any AWS alerts or anomalies?\n",
            "* Any ongoing maintenance activities?\n",
            "* Any blue stack refresh activities?\n",
            "* Correlate the order dip time with any known activities.\n",
            "\n",
            "Task Name:\n",
            "* Low order Sanity AO\n",
            "* Validate External API Response time\n",
            "* TMO Sanity AO\n",
            "\n",
            "Note: The task names are listed as a comma separated Python list.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time = 58601.99 ms\n",
            "llama_print_timings:      sample time =   619.79 ms /   538 runs   (    1.15 ms per token,   868.04 tokens per second)\n",
            "llama_print_timings: prompt eval time = 122702.74 ms /  1037 tokens (  118.32 ms per token,     8.45 tokens per second)\n",
            "llama_print_timings:        eval time = 90671.89 ms /   537 runs   (  168.85 ms per token,     5.92 tokens per second)\n",
            "llama_print_timings:       total time = 216090.16 ms\n"
          ]
        }
      ],
      "source": [
        "query = \"Find any automation that can be used for query : 'What are the steps for low order trend scenerio'. You MUST answer using the following json schema:\"\n",
        "question_with_schema = f'{query}{AnswerFormat.schema_json()}'\n",
        "res = qa(question_with_schema)\n",
        "answer, docs = res[\"result\"], res[\"source_documents\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d179c65b-4d0b-4415-8919-f002b8fffc21",
      "metadata": {
        "id": "d179c65b-4d0b-4415-8919-f002b8fffc21"
      },
      "outputs": [],
      "source": [
        "output_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "task_name: Extract any automation task name that can be used for the user query and output them as a comma separated Python list.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "task_name\n",
        "\n",
        "text: {text}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d52f96-e173-40d0-bdda-bbb2f63a6005",
      "metadata": {
        "id": "52d52f96-e173-40d0-bdda-bbb2f63a6005",
        "outputId": "d85512b6-7c23-4514-ac63-0733aa1d7b41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_variables=['text'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='For the following text, extract the following information:\\n\\ntask_name: Extract any automation task name that can be used for the user query and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ntask_name\\n\\ntext: {text}\\n'))]\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(output_template)\n",
        "print(prompt_template)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "41384b5513b340f8802f50c025b92825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e708affcac6f4b3eb372e70dd7c99879",
              "IPY_MODEL_5abc3467a24449058e52f0ff8c8472de",
              "IPY_MODEL_352134394aa446129d9ba29e06c160af"
            ],
            "layout": "IPY_MODEL_3ab4d94f9792415485396b7137296608"
          }
        },
        "e708affcac6f4b3eb372e70dd7c99879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_841174c277e9438fa827eee1afa575cc",
            "placeholder": "​",
            "style": "IPY_MODEL_650e3d4084b146aca2605cac172ce4cb",
            "value": "Downloading (…)-7b-chat.Q4_K_M.gguf: 100%"
          }
        },
        "5abc3467a24449058e52f0ff8c8472de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e088071194d438d8ee25c18e25f1cb1",
            "max": 4081004224,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9efe5e4197b4c778bdfc5c18430e694",
            "value": 4081004224
          }
        },
        "352134394aa446129d9ba29e06c160af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21f5912b1c6947ffbf68d6ddfa894ba0",
            "placeholder": "​",
            "style": "IPY_MODEL_34d6762056544310a034f1cf7f9cfa77",
            "value": " 4.08G/4.08G [00:36&lt;00:00, 246MB/s]"
          }
        },
        "3ab4d94f9792415485396b7137296608": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "841174c277e9438fa827eee1afa575cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "650e3d4084b146aca2605cac172ce4cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e088071194d438d8ee25c18e25f1cb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9efe5e4197b4c778bdfc5c18430e694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21f5912b1c6947ffbf68d6ddfa894ba0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34d6762056544310a034f1cf7f9cfa77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}