# -*- coding: utf-8 -*-
"""llama-13b

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15qugUBumOwy8agHaVoHctcQTVkQfMBWV

# About

- Use [Langchain](https://python.langchain.com/en/latest/index.html) to build a chatbot
- Experiment with various LLMs (Large Language Models)
- Use [ChromaDB vector store](https://python.langchain.com/docs/integrations/vectorstores/chroma) to store text embeddings with [Instructor-Finetuned Text Embeddings](https://arxiv.org/pdf/2212.09741.pdf) from [Hugging Face](https://huggingface.co/hkunlp/instructor-large)
- Use [Retrieval chain](https://python.langchain.com/docs/modules/data_connection/retrievers/) to retrieve relevant passages from embedded text
- Summarize retrieved passages

No need to create any API key to use this notebook! Everything is open source.

Upvote the notebook if you learn from it or use it! :)

This will help me keep experimenting with new models as soon as they are released

### Models

- [WizardLM](https://huggingface.co/TheBloke/wizardLM-7B-HF)
- [Falcon](https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2)
- [Llama 2-7b](https://huggingface.co/daryl149/llama-2-7b-chat-hf)
- [Llama 2-13b](https://huggingface.co/daryl149/llama-2-13b-chat-hf)
- [Bloom](https://huggingface.co/bigscience/bloom-7b1)

![image.png](attachment:4dc05295-4765-45ef-88c3-a9be30a35320.png)

img source: HinePo
"""

! nvidia-smi -L

"""# Installs"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# ! pip install -qq -U langchain tiktoken pypdf chromadb faiss-gpu
# ! pip install -qq -U transformers InstructorEmbedding sentence_transformers
# ! pip install -qq -U accelerate bitsandbytes xformers einops

"""# Imports"""

import warnings
warnings.filterwarnings("ignore")

import os
import glob
import textwrap
import time

import langchain

# loaders
from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import DirectoryLoader

# splits
from langchain.text_splitter import RecursiveCharacterTextSplitter

# prompts
from langchain import PromptTemplate, ConversationChain, LLMChain

# vector stores
from langchain.vectorstores import Chroma, FAISS

# models
from langchain.llms import HuggingFacePipeline
from InstructorEmbedding import INSTRUCTOR
from langchain.embeddings import HuggingFaceInstructEmbeddings

# retrievers
from langchain.chains import RetrievalQA, ConversationalRetrievalChain

import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

print(langchain.__version__)

"""# CFG

- CFG class enables easy and organized experimentation
"""

class CFG:
    # LLMs
    model_name = 'llama2-13b' # wizardlm, bloom, falcon, llama2-7b, llama2-13b
    temperature = 0,
    top_p = 0.95,
    repetition_penalty = 1.15

    # splitting
    split_chunk_size = 800
    split_overlap = 0

    # embeddings
    embeddings_model_repo = 'hkunlp/instructor-base'

    # similar passages
    k = 3

    # paths
    PDFs_path = 'data'
    Embeddings_path =  'embeddings/vectordb-chroma'
    Persist_directory = './vectordb-chroma'

"""# Define model"""

def get_model(model = CFG.model_name):

    print('\nDownloading model: ', model, '\n\n')

    if model == 'wizardlm':
        model_repo = 'TheBloke/wizardLM-7B-HF'

        tokenizer = AutoTokenizer.from_pretrained(model_repo)

        model = AutoModelForCausalLM.from_pretrained(
            model_repo,
            load_in_4bit=True,
            device_map='auto',
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True
            )

        max_len = 1024

    elif model == 'llama2-7b':
        model_repo = 'daryl149/llama-2-7b-chat-hf'

        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)

        model = AutoModelForCausalLM.from_pretrained(
            model_repo,
            load_in_4bit=True,
            device_map='auto',
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            trust_remote_code=True
            )

        max_len = 2048

    elif model == 'llama2-13b':
        model_repo = 'daryl149/llama-2-13b-chat-hf'

        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)

        model = AutoModelForCausalLM.from_pretrained(
            model_repo,
            load_in_4bit=True,
            device_map='auto',
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            max_memory={0: "10GB"}
            )

        max_len = 8192

    elif model == 'bloom':
        model_repo = 'bigscience/bloom-7b1'

        tokenizer = AutoTokenizer.from_pretrained(model_repo)

        model = AutoModelForCausalLM.from_pretrained(
            model_repo,
            load_in_4bit=True,
            device_map='auto',
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            )

        max_len = 1024

    elif model == 'falcon':
        model_repo = 'h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2'

        tokenizer = AutoTokenizer.from_pretrained(model_repo)

        model = AutoModelForCausalLM.from_pretrained(
            model_repo,
            load_in_4bit=True,
            device_map='auto',
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            trust_remote_code=True
            )

        max_len = 1024

    else:
        print("Not implemented model (tokenizer and backbone)")

    return tokenizer, model, max_len

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# tokenizer, model, max_len = get_model(model = CFG.model_name)





"""# ðŸ¤— pipeline

- Hugging Face pipeline
"""

pipe = pipeline(
    task = "text-generation",
    model = model,
    tokenizer = tokenizer,
    pad_token_id = tokenizer.eos_token_id,
    max_length = max_len,
    temperature = CFG.temperature,
    top_p = CFG.top_p,
    repetition_penalty = CFG.repetition_penalty
)

llm = HuggingFacePipeline(pipeline = pipe)

llm

### testing model
query = "hi"
llm(query)

"""# ðŸ¦œðŸ”— Langchain

- Multiple document retriever with LangChain
"""

CFG.model_name

"""## Loader

- [Directory loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/file_directory) for multiple files
- This step is not necessary if you are just loading the vector database
- This step is necessary if you are creating embeddings. In this case you need to:
    - load de PDF files
    - split into chunks
    - create embeddings
    - save the embeddings in a vector store
    - After that you can just load the saved embeddings to do similarity search with the user query, and then use the LLM to answer the question
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# loader = DirectoryLoader(CFG.PDFs_path,
#                          glob="./*.pdf",
#                          loader_cls=PyPDFLoader,
#                          show_progress=True,
#                          use_multithreading=True)
# 
# documents = loader.load()

len(documents)

documents[2].page_content

"""## Splitter

- Splitting the text into chunks so its passages are easily searchable for similarity
- This step is also only necessary if you are creating the embeddings
- [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/reference/modules/document_loaders.html?highlight=RecursiveCharacterTextSplitter#langchain.document_loaders.MWDumpLoader)
"""

text_splitter = RecursiveCharacterTextSplitter(chunk_size = CFG.split_chunk_size,
                                               chunk_overlap = CFG.split_overlap)

texts = text_splitter.split_documents(documents)
len(texts)

"""## Embeddings

- Embedd and store the texts in a Vector database (ChromaDB or FAISS)
- [LangChain Vector Stores docs](https://python.langchain.com/docs/modules/data_connection/vectorstores/)
- [One Embedder, Any Task: Instruction-Finetuned Text Embeddings - paper Dec/2022](https://arxiv.org/pdf/2212.09741.pdf)
- [This is a nice 4 minutes video about vector stores](https://www.youtube.com/watch?v=dN0lsF2cvm4)
- [Persist and load the vector database](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html)

On this Harry Potter dataset, this embeddings creation and storage takes about ~35 minutes to complete with this embeddings function, configuration and compute power.

We need to create the embeddings only once, and then we can just load the vector store and query the database using similarity search.

Loading the embeddings takes only a few seconds.

I uploaded the embeddings to a Kaggle Dataset so we just load it from [here](https://www.kaggle.com/datasets/hinepo/hp-embeddings-instructor-base-800-0).

## Create vector database
"""

CFG.embeddings_model_repo

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# ### download embeddings model
# instructor_embeddings = HuggingFaceInstructEmbeddings(model_name = CFG.embeddings_model_repo,
#                                                       model_kwargs = {"device": "cuda"})
# 
# ### create embeddings and DB
# vectordb = Chroma.from_documents(documents = texts,
#                                  embedding = instructor_embeddings,
#                                  persist_directory = CFG.Persist_directory,
#                                  collection_name = 'resumes')
# 
# # vectordb.add_documents(documents=texts, embedding=instructor_embeddings)
# 
# ### persist Chroma vector database
# vectordb.persist()

"""## Load vector database

- After persisting the vector database, we just load it from the Kaggle Dataset I mentioned
- Obviously, the embeddings function to load the embeddings must be the same as the one used to create the embeddings
- After some experimentation I found out that there is a compatibility issue between Kaggle Public Datasets and Chroma. Loading the stored vector database as a Public Dataset does not work. It only works if the Dataset is Private. So you can either:
    - Create your own embeddings, save your vector store and then load it; or
    - Download the embeddings I've already created, upload them to your kaggle account (keeping it as a Private dataset), and load them in this notebook. This will work.
    - This compatibility issue does not happen in Google Colab.
"""

# %%time

# ### download embeddings model
# instructor_embeddings = HuggingFaceInstructEmbeddings(model_name = CFG.embeddings_model_repo,
#                                                       model_kwargs = {"device": "cuda"})

# vectordb = Chroma(persist_directory = CFG.Embeddings_path,
#                   embedding_function = instructor_embeddings,
#                   collection_name = 'resumes')

### how many documents were loaded
print(vectordb._collection.count())

"""# Prompt Template

- Custom prompt
"""

prompt_template = """
Don't try to make up an answer, if you don't know just say that you don't know.
Answer in the same language the question was asked.
Use only the following pieces of context to answer the question at the end.

{context}

Question: {question}
Answer:"""


PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

# llm_chain = LLMChain(prompt=PROMPT,
#                      llm=llm)
# llm_chain

"""# Retriever chain

- Retriever to retrieve relevant passages
- Chain to answer questions
- [RetrievalQA: Chain for question-answering](https://python.langchain.com/docs/modules/data_connection/retrievers/)
"""

retriever = vectordb.as_retriever(search_kwargs = {"k": CFG.k, "search_type" : "similarity"})

qa_chain = RetrievalQA.from_chain_type(llm = llm,
                                       chain_type = "stuff", # map_reduce, map_rerank, stuff, refine
                                       retriever = retriever,
                                       chain_type_kwargs = {"prompt": PROMPT},
                                       return_source_documents = True,
                                       verbose = False)

### testing MMR search
question = "TELL ME ABOUT JOSEPH"
vectordb.max_marginal_relevance_search(question, k = CFG.k)

"""## Post-process outputs

- Format llm response
- Cite sources (PDFs)
"""

def wrap_text_preserve_newlines(text, width=200): # 110
    # Split the input text into lines based on newline characters
    lines = text.split('\n')

    # Wrap each line individually
    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]

    # Join the wrapped lines back together using newline characters
    wrapped_text = '\n'.join(wrapped_lines)

    return wrapped_text

def process_llm_response(llm_response):
    ans = wrap_text_preserve_newlines(llm_response['result'])
    sources_used = ' \n'.join([str(source.metadata['source']) for source in llm_response['source_documents']])
    ans = ans + '\n\nSources: \n' + sources_used
    return ans

def llm_ans(query):
    start = time.time()
    llm_response = qa_chain(query)
    ans = process_llm_response(llm_response)
    end = time.time()

    time_elapsed = int(round(end - start, 0))
    time_elapsed_str = f'\n\nTime elapsed: {time_elapsed} s'
    return ans + time_elapsed_str

"""# Ask questions

- Question Answering from multiple documents
- Run QA Chain
- Talk to your data
"""

CFG.model_name

model

query = "Please suggest some candidate who is good for datascience role ?"
print(llm_ans(query))

query = "From the resumes you have please suggest best candidate suitable for a datascience role "
print(llm_ans(query))

query = "What training we can suggest to Joseph Thomas , for a datascience role ? Please suggest some courses also "
print(llm_ans(query))

"""# Gradio Chat UI

- Create a chat UI with [Gradio](https://www.gradio.app/guides/quickstart)
- [ChatInterface docs](https://www.gradio.app/docs/chatinterface)
- The notebook should be running if you want to use the chat interface
- Print of the chat UI below
"""

! pip install gradio -qq

import gradio as gr

def predict(message, history):
    # output = message # debug mode

    output = str(llm_ans(message))
    return output

demo = gr.ChatInterface(predict,
                        title = f' Open-Source LLM ({CFG.model_name}) for Harry Potter Question Answering')

demo.launch()

"""![image.png](attachment:8b4e495c-2345-4c6a-9d5d-a5a256443354.png)

# Conclusions

- Feel free to fork and optimize the code. Lots of things can be improved. I'm planning to experiment with Memory soon.

- Things I found had the most impact on models output quality in my experiments:
    - Prompt engineering
    - Splitting: chunk size, overlap
    - Search: Similarity, MMR , k
    - Pipeline parameters (top_p, penalty)
    - Embeddings function
    - LLM parameters (max len)
    - Other models families
    - Bigger models


- LangChain, Hugging Face and Gradio are awesome libs!

- Upvote if you liked it or want me to keep updating this with new models and functionalities!

ðŸ¦œðŸ”—ðŸ¤—

![image.png](attachment:68773819-4358-4ded-be3e-f1d275103171.png)
"""
