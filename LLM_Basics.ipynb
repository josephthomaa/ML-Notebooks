{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxktC2dFtz1NhzGxtmA6rs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josephthomaa/ML-Notebooks/blob/main/LLM_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###  Large language model (LLM)\n",
        "\n",
        "A large language model (LLM) is a type of artificial intelligence (AI) model that has been trained on a massive dataset of text and code. This training allows the LLM to learn the statistical relationships between words and phrases, which in turn allows it to generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\n",
        "\n",
        "LLMs are typically trained using a technique called **self-supervised learning**. In self-supervised learning, the model is not given any explicit labels for the data it is trained on. Instead, the model is given a task that it can perform without any labels, such as predicting the next word in a sentence or filling in the blanks in a text. The model learns to perform this task by finding patterns in the data that correlate with the desired output.\n",
        "\n",
        "The size of an LLM is typically measured by the number of parameters it has. A parameter is a variable that the model can change as it learns. The more parameters an LLM has, the more complex the model can be and the better it can perform on natural language processing tasks.\n",
        "\n",
        "Some of the most successful LLMs have hundreds of billions of parameters. These models are trained on massive datasets of text and code, which can be terabytes or even petabytes in size. The training of these models requires a lot of computing power, and it can take weeks or even months to complete."
      ],
      "metadata": {
        "id": "qXCUie1qDVfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformers** provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio by hugging face.\n",
        "\n",
        "examples :  \n",
        "  \n",
        "\n",
        "1.  LlamaForCausalLM: This module provides a class for loading and using a pre-trained language model for causal language modeling tasks, specifically the Llama model.\n",
        "2.  LlamaTokenizer: This module provides a class for loading and using a pre-trained tokenizer specifically for the Llama model.\n",
        "3.  pipeline: This module provides a function for creating a pipeline for a specific task, such as text generation or question answering.\n",
        "\n"
      ],
      "metadata": {
        "id": "NIP1ReI4O-1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RalMWlaaCcTH"
      },
      "outputs": [],
      "source": [
        "#\n",
        "\n",
        "# !pip install transformers\n",
        "# https://github.com/huggingface/transformers\n",
        "\n",
        "# The most basic object in the ðŸ¤— Transformers library is the pipeline() function.\n",
        "# It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer\n",
        "# https://huggingface.co/docs/transformers/main_classes/pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Pretraining** is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge.\n",
        "\n",
        "\n",
        "**Fine-tuning**, on the other hand, is the training done after a model has been pretrained. To perform fine-tuning, you first acquire a pretrained language model, then perform additional training with a dataset specific to your task.\n",
        "\n"
      ],
      "metadata": {
        "id": "0crn3hopI6bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**General architecture** of the Transformer model\n",
        "  >[general-architecture](https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt#general-architecture)\n",
        "\n",
        "\n",
        "Model Types : Encoder, Decoder, Encoder-decoder\n",
        "\n",
        "  [Understanding Encoder-Decoder Sequence to Sequence Model](https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346)\n",
        "  "
      ],
      "metadata": {
        "id": "UEFc7KIoJ-rW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### langchain\n",
        "\n",
        "LangChain is an open-source framework for building applications with large language models (LLMs). It provides a standard interface for chains, which are sequences of calls to LLMs and other utilities. LangChain also provides a variety of features for building and deploying applications, such as chains, integrations, memory etc .."
      ],
      "metadata": {
        "id": "y6vRvcijQQYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @misc{huggingfacecourse,\n",
        "#   author = {Hugging Face},\n",
        "#   title = {The Hugging Face Course, 2022},\n",
        "#   howpublished = \"\\url{https://huggingface.co/course}\",\n",
        "#   year = {2022},\n",
        "#   note = \"[Online; accessed <today>]\"\n",
        "# }"
      ],
      "metadata": {
        "id": "W-_S6y6xC_G3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K2p6FTkTFctQ"
      }
    }
  ]
}